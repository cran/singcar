<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>The R package singcar: Comparing Single Cases to Small Samples</title>


<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>

<style type="text/css">
p.abstract{
text-align: center;
font-weight: bold;
}
div.abstract{
margin: auto;
width: 90%;
}
</style>


<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">The R package singcar: Comparing Single Cases to Small Samples</h1>
<h4 class="author">Jonathan Ö. Rittmo</h4>
<address class="author_afil">
University of Edinburgh<br><a class="author_email" href="mailto:#"><a href="mailto:j.rittmo@gmail.com" class="email">j.rittmo@gmail.com</a></a>
</address>
<h4 class="author">Robert D. McIntosh</h4>
<address class="author_afil">
University of Edinburgh<br><a class="author_email" href="mailto:#"><a href="mailto:r.d.mcintosh@ed.ac.uk" class="email">r.d.mcintosh@ed.ac.uk</a></a>
</address>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>Statistical comparison of single cases to small samples is a methodology that has been extensively used in, for example, cognitive and clinical neuropsychology. This is most often done to determine changes in cognitive processing after an individual has incurred some type of brain damage. In a clinical setting one often wish to infer whether a patient exhibit abnormally low performance on some cognitive ability. In cognitive neuropsychology on the other hand one often wish to infer if a patient exhibits an abnormally large discrepancy in performance between two cognitive abilities. Because cognitive abilities seldom are well represented by one single performance on one single task one might additionally be interested in the abnormality of a case on several measurements converging on a cognitive ability of interest, or the abnormality of a case in a multivariate space. Several methods to estimate case abnormality have been developed that keeps the Type I error rate at its nominal level. However, they have not been available in any standard statistical software environment and their documentation is spread thin across multiple articles and compiled computer programs. This vignette aims to gather and review the most popular methods while presenting them and their usage in the <code>R</code> package <code>singcar</code>. Of note are the more flexible and useful methods that have not received as much spread as the simpler. These include techniques using Bayesian regression to allow for the inclusion of covariates and linear mixed models to handle repeated measures data. Additionally, statistical comparison of single cases to a control population are inherently low powered. To facilitate experimental planning and design power calculators have been implemented in <code>singcar</code> and the concept of power for this type of statistical analysis is reviewed.</p>
</div>



<div id="section1" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>There might be several needs to probabilistically determine
if a single observation belongs to some population distribution.
In some areas, such as neuropsychology, this methodology has been
essential for the advancement of the field. This is because brain-damaged patients
with patterns of associated cognitive functional impairments often are unique in each case.
A single patient might then be the only available source of data for
a phenomenon of interest. In addition, clinical diagnoses and description
of patient impairments are always performed for single cases.</p>
<p>Patients with brain damage are hence often compared to a distribution of
cognitive functioning in the healthy population,
both in research and clinical contexts. We do however not always know
the population parameters of such a distribution, and when we do not,
these must be estimated from a sample – we compare the single case
to a control sample. There are other potential application areas
of this method such as studies of uncommon expertise, targeted quality
checks in small production industries or some animal studies
where rearing large experimental groups might be too costly or unethical.</p>
<p>However, since these methods most commonly are used within the field
of neuropsychology the related nomenclature will be adopted. Abnormally
low score on some variate compared to the control sample will be referred
to as a <em>deficit</em>, important both in clinics and in research. For the latter
another concept is also considered to be of great interest: namely an abnormally
large discrepancy between two variates. This is known as a <em>dissociation</em> and
provides evidence for independence between cognitive abilities. By mapping
such discrepancies it is possible to build theories of the architecture of the
human mind <span class="citation">(<a href="#ref-shalliceNeuropsychologyMentalStructure1988" role="doc-biblioref">Shallice 1988</a>)</span>.</p>
<p>During the last 20 years methods allowing researchers to estimate abnormality and test
for deficits and dissociations with retained control of Type I errors have
been developed. This has mainly been done by John Crawford and Paul
Garthwaite, e.g. <span class="citation">Crawford and Howell (<a href="#ref-crawfordComparingIndividualTest1998" role="doc-biblioref">1998</a>)</span>, <span class="citation">Crawford, Garthwaite, and Ryan (<a href="#ref-crawfordComparingSingleCase2011" role="doc-biblioref">2011</a>)</span>, <span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordComparisonSingleCase2007" role="doc-biblioref">2007</a>)</span>, <span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordInvestigationSingleCase2002" role="doc-biblioref">2002</a>)</span> and <span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordTestingSuspectedImpairments2005" role="doc-biblioref">2005</a>)</span>.
These methods have been implemented in freely available software.
However, only as standalone programs, not open source and only for Windows.
Most of the programs require manual input of summary statistics and cannot
be used on raw data. Full documentation of the implemented methods are only
available in the original publications.</p>
<p>This was the motivation behind the development of the <code>R</code> package <code>singcar</code>
<span class="citation">(<a href="#ref-rittmoSingcarComparingSingle2021" role="doc-biblioref">Rittmo and McIntosh 2021</a>)</span>. We wanted to encourage and simplify the usage of these
methods by bringing them together in a fully documented package with open source
code that works across platforms. In Section <a href="#section2">2</a> all implemented methods
will be described in detail. This contribution will provide a comprehensive
overview of the methods available together with their advantages and disadvantages.</p>
<p>The development of Crawford and Garthwaite’s methods has been focused around
limiting Type I errors. But in recent work we showed the inherent inability of
controlling Type II errors using this experimental design and argued that this
limitation might even be more detrimental than an increase Type I error rate,
for some applications of the methods <span class="citation">(<a href="#ref-mcintoshPowerCalculationsSinglecase2020" role="doc-biblioref">McIntosh and Rittmo 2020</a>)</span>.
Therefore we also provide power calculators for each test function in the
package.</p>
<p>When we conduct single case comparisons we want to estimate the probability that
a random draw from the distribution estimated by the sample is more extreme than
the single observation we are interested in. This means that the power of a case
comparison hypothesis test is contingent on the standard deviation of the
distribution, not on the standard error of the test statistic. Increasing the
sample size then (which usually is done to increase power) only leads to a
better estimation of the control distribution, not a reduction of the the
standard error. Hence, the limit of achievable power is mostly contingent on the
size of the effect. When investigating unique cases it is not always possible to
target larger effects to get rid of this problem.</p>
<p>One approach that often is highlighted for its potential to increase power is
multivariate hypothesis testing. This is however only true to a certain extent
<span class="citation">(<a href="#ref-franePowerTypeError2015" role="doc-biblioref">Frane 2015</a>)</span> and one should not force a multivariate analysis on a
univariate research question. Focus hitherto has been univariate in
nature, but for a neuropsychological dissociation to be convincing the optimal
experimental design would include multiple tasks converging on the cognitive
functions of interest, performed on more than one occasion
<span class="citation">(<a href="#ref-shalliceNeuropsychologyMentalStructure1988" role="doc-biblioref">Shallice 1988</a>; <a href="#ref-mcintoshPowerCalculationsSinglecase2020" role="doc-biblioref">McIntosh and Rittmo 2020</a>)</span>. Integrating such information
both across tasks as well as over several performances could be aided by
multivariate analyses.</p>
<p>This calls for the need of methods that estimate a case’s abnormality in a
multidimensional space estimated by a sample. The most obvious way to do this
would of course entail the <span class="math inline">\(p\)</span> value of a Hotelling’s <span class="math inline">\(T^2\)</span>-score. However,
<span class="citation">Elfadaly, Garthwaite, and Crawford (<a href="#ref-elfadalyPointEstimationAbnormality2016" role="doc-biblioref">2016</a>)</span> show that this statistic is
substantially biased as a measure of abnormality. They investigate nine other candidates including two point
estimates based on polynomial expansion, which gave rise to the lowest bias.
These point estimates would, however, sometimes yield estimates out of the [0,
1] range and their interpretation is not very intuitive. Because ease of
interpretation is paramount in scientific reporting
<span class="citation">Elfadaly, Garthwaite, and Crawford (<a href="#ref-elfadalyPointEstimationAbnormality2016" role="doc-biblioref">2016</a>)</span> suggested using an adapted median
estimator when bias is not of “immediate importance”, in which case the
polynomial estimators should be used. A compiled computer program
for this adapted median estimator is available <span class="citation">(<a href="#ref-garthwaiteModifiedConfidenceIntervals2017" role="doc-biblioref">Garthwaite, Elfadaly, and Crawford 2017</a>)</span>,
but again, no open source software working across platforms.
This relatively novel estimation method has also been implemented in
<code>singcar</code>.</p>
<p>The following section, Section <a href="#section2">2</a>, will review available
methods for single case comparisons, their theory and how they can be
used in <code>R</code>. Section <a href="#power">2.5.3</a> gives a brief review of statistical
power in this experimental design and the implementation of power
calculators in <code>singcar</code>, but for a more thorough and perhaps
more accessible review see <span class="citation">McIntosh and Rittmo (<a href="#ref-mcintoshPowerCalculationsSinglecase2020" role="doc-biblioref">2020</a>)</span>.</p>
</div>
<div id="section2" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Comparing a single case to small samples</h1>
<div id="background" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Background</h2>
<p>Variates of interest will in a neuropsychological context often be scores on
some task assessing a cognitive function. Therefore they will often be referred
to as “task scores”. It has not been uncommon in this field to compare patients
to a control sample, treating the sample statistics as population parameters and
estimating deficits by evaluating the <span class="math inline">\(p\)</span> value associated with the patient’s
<span class="math inline">\(Z\)</span> score from the estimated distribution. Estimating dissociations was done in
a similar way using a method developed by
<span class="citation">Payne and Jones (<a href="#ref-payneStatisticsInvestigationIndividual1957" role="doc-biblioref">1957</a>)</span>, where a <span class="math inline">\(Z\)</span> score is calculated
from the distribution of difference scores in the control sample.</p>
<p>This is of course problematic if small samples are used, which is often the case
in neuropsychology, since the sampling distribution of the sample variance
is right skewed for small sample sizes. If one derives a test statistic
that is divided by a skewed distribution the size of the obtained values
would be inflated and thus the Type I error rate <span class="citation">(<a href="#ref-crawfordComparingIndividualTest1998" role="doc-biblioref">Crawford and Howell 1998</a>)</span>.</p>
<p>In the following sections I will describe methods that has been devised
as replacements to the <span class="math inline">\(Z\)</span> score methods. Both frequentist and Bayesian
significance tests have been developed and are covered in Section <a href="#sec22">2.2</a>
and Section <a href="#sec23">2.3</a> respectively. More complex model designs can be
achieved using Bayesian regression techniques, described in Section <a href="#cov">2.3.4</a>,
or linear mixed models, described in Section <a href="#lmm">2.4</a>. For estimating
multivariate abnormality, see Section <a href="#section4">2.5</a>.</p>
<div id="exdata" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Example data</h3>
<p>In the following sections the theory behind the methods in <code>singcar</code> is
reviewed. But at the end of each subsection the usage of the implementations
will be exemplified. To aid this the package comes with the dataset
<code>size_weight_illusion</code>, which is a dataset from a neuropsychological study
investigating the size-weight illusion in a well known patient “DF”
<span class="citation">(<a href="#ref-hassanSizeweightIllusionVisual2020" role="doc-biblioref">Hassan et al. 2020</a>)</span>. DF incurred damage to the lateral
occipital complex which gave rise to visual form agnosia, potentially making her less
susceptible to the size-weight illusion <span class="citation">(<a href="#ref-dijkermanVisuomotorPerformancePatient2004a" role="doc-biblioref">Dijkerman et al. 2004</a>)</span>.
This illusion is a perceptual phenomenon making objects of small size be
perceived as heavier during lifting than objects of larger size but equal in
weight <span class="citation">(<a href="#ref-buckinghamGettingGripHeaviness2014" role="doc-biblioref">Buckingham 2014</a>)</span>.</p>
<p>However, due to the location of DF’s brain damage one would only suspect
less susceptibility to the illusion for visual cues since she other sensory
processing should be unaffected of her damage. In the study by <span class="citation">Hassan et al. (<a href="#ref-hassanSizeweightIllusionVisual2020" role="doc-biblioref">2020</a>)</span>
the illusion is tested given visual cues in one condition and kinaesthetic
cues in another. It was predicted that she would be abnormally less susceptible to
visual size-weight illusion and that there would be an abnormally large discrepancy
between visual and kinaesthetic size-weight illusion. The control sample consisted
of 28 participants and their age and sex was collected along with their performance
in the two conditions. The size-weight illusion is given in the dataset as a
scaled measure of the number of grams weight difference perceived per cubic cm
of volume change. To use this data, start by installing and loading the package:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;singcar&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;singcar&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(size_weight_illusion) </span></code></pre></div>
<pre><code>##   GROUP PPT    SEX YRS      V_SWI      K_SWI
## 1    SC  DF Female  65 0.02814921 0.10012712
## 2    HC E01 Female  70 0.21692634 0.21792930
## 3    HC E02   Male  64 0.17223226 0.26338899
## 4    HC E03 Female  63 0.07138049 0.09331700
## 5    HC E04 Female  65 0.10186453 0.25938045
## 6    HC E05 Female  66 0.15911439 0.08922615</code></pre>
<p><code>V_SWI</code> and <code>K_SWI</code> are the measurements for visual and kinaesthetic
size-weight illusion respectively. Most functions in <code>singcar</code>
can take summary data as input, but for demonstrative purposes the raw
data of this dataset will be used. To illustrate usage
of the methods more generally, the scores of the variates of interest
as well as of the covariate <code>YRS</code> is extracted from the data and renamed:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>caseA <span class="ot">&lt;-</span> size_weight_illusion[<span class="dv">1</span>, <span class="st">&quot;V_SWI&quot;</span>]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>contA <span class="ot">&lt;-</span> size_weight_illusion[<span class="sc">-</span><span class="dv">1</span>, <span class="st">&quot;V_SWI&quot;</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>caseB <span class="ot">&lt;-</span> size_weight_illusion[<span class="dv">1</span>, <span class="st">&quot;K_SWI&quot;</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>contB <span class="ot">&lt;-</span> size_weight_illusion[<span class="sc">-</span><span class="dv">1</span>, <span class="st">&quot;K_SWI&quot;</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>caseAGE <span class="ot">&lt;-</span> size_weight_illusion[<span class="dv">1</span>, <span class="st">&quot;YRS&quot;</span>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>contAGE <span class="ot">&lt;-</span> size_weight_illusion[<span class="sc">-</span><span class="dv">1</span>, <span class="st">&quot;YRS&quot;</span>]</span></code></pre></div>
<p>Here, <code>caseA</code> and <code>caseB</code> refers to the case scores on some task A
and B respectively. Similarly, <code>contA</code> and <code>contB</code> refers to the scores
of the control sample on the same tasks.</p>
</div>
</div>
<div id="sec22" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Frequentist approaches</h2>
<p>When we are comparing a patient to a healthy population on some normally
distributed variate that we are estimating from a small sample, we are in
fact not estimating abnormality from a normal distribution but a central
<span class="math inline">\(t\)</span> distribution.</p>
<p>Hence, we must use a <span class="math inline">\(t\)</span> score rather than a <span class="math inline">\(Z\)</span> score if we want to estimate
deficits without inflating effect sizes. This was noted by
<span class="citation">Sokal and Rohlf (<a href="#ref-sokalBiometryPrinciplesPractice1981" role="doc-biblioref">1981</a>)</span> (p. 227) where they devised a test
statistic for single cases in the biological sciences, but it was first popularised
within neuropsychology by <span class="citation">Crawford and Howell (<a href="#ref-crawfordComparingIndividualTest1998" role="doc-biblioref">1998</a>)</span>. Its common
application is as a one-tailed “test of deficit” (TD) that estimate the
proportion of the healthy population that would exhibit a lower score than the
case on some cognitive task. If this score falls below some chosen threshold the
patient can be diagnosed with a deficit.</p>
<p>This basic approach is simply a modified two samples <span class="math inline">\(t\)</span> test,</p>
<p><span class="math display" id="eq:TD">\[\begin{equation}
t_{n-1} = \frac{y^* - \overline{y}}{s \sqrt{\frac{n + 1}{n}}}
\tag{2.1}
\end{equation}\]</span></p>
<p>where
one of the samples are treated as a size of <span class="math inline">\(n = 1\)</span>. The degrees of freedom
then quite naturally become <span class="math inline">\(n+1-2 = n -1\)</span>.
<span class="math inline">\(\overline{y}, \ s\)</span> and <span class="math inline">\(n\)</span> are the sample mean, standard deviation and size and
<span class="math inline">\(y^*\)</span> is the case score. This method allows transparent control over the Type I
error rate <span class="citation">(<a href="#ref-crawfordComparingSingleCase2009" role="doc-biblioref">Crawford, Garthwaite, and Howell 2009</a>; <a href="#ref-crawfordInferentialMethodsComparing2004" role="doc-biblioref">Crawford et al. 2004</a>; <a href="#ref-crawfordSinglecaseResearchNeuropsychology2012" role="doc-biblioref">Crawford and Garthwaite 2012</a>)</span> when used for significance
testing. In addition, the <span class="math inline">\(p\)</span> value associated with he obtained <span class="math inline">\(t\)</span> statistic
gives an unbiased point estimate of the abnormality of the case. That is, it
provides us with the expected percentage of the normal population that would
exhibit a more extreme score than the case. This estimate is often
of main interest in neuropsychology. The simple proof for this is
demonstrated by <span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordMethodsTestingDeficit2006" role="doc-biblioref">2006</a>)</span> as outlined below.</p>
<p>The percentage of a population expected to score lower on some variate <span class="math inline">\(Y\)</span> than
the case <span class="math inline">\(y^*\)</span> is
<span class="math display">\[
\mathbb{P}[Y&lt; y^*].
\]</span>
Subtracting <span class="math inline">\(\overline{y}\)</span> from both sides of the inequality
and dividing by <span class="math inline">\(s \sqrt{\frac{n + 1}{n}}\)</span> gives
<span class="math display">\[
\mathbb{P}[Y&lt; y^*] =\mathbb{P}\left[\frac{Y-\overline{y}}{s \sqrt{\frac{n + 1}{n}}} &lt; \frac{y^* - \overline{y}}{s \sqrt{\frac{n + 1}{n}}}\right].
\]</span>
The quantity to the left of the inequality, i.e., <span class="math inline">\(\frac{y-\overline{y}}{s \sqrt{\frac{n + 1}{n}}}\)</span>
is <span class="math inline">\(t\)</span> distributed with <span class="math inline">\(n-1\)</span> degrees of freedom. Hence,
<span class="math display">\[
\mathbb{P}[Y&lt; y^*] =\mathbb{P}\left[t_{n-1} &lt; \frac{y^* - \overline{y}}{s \sqrt{\frac{n + 1}{n}}}\right].
\]</span>
To the right of the inequality we have the test statistic from
(<a href="#eq:TD">(2.1)</a>). Therefore, <span class="math inline">\(\mathbb{P}[y&lt; y^*]\)</span> is
the <span class="math inline">\(p\)</span> value from the test of deficit. This simple fact also
makes the construction of confidence intervals for the abnormality (i.e. <span class="math inline">\(p\)</span>)
possible.</p>
<p>Although the <span class="math inline">\(Z\)</span> score is not an appropriate statistic to use for
significance testing when the control sample is small, it does
provide a standardised effect measure of the deficit of interest
similar to Cohen’s <span class="math inline">\(d\)</span>, because insensitivity to sample size is a requirement
for effect size measures <span class="citation">(<a href="#ref-cohenStatisticalPowerAnalysis1988" role="doc-biblioref">Cohen 1988</a>; <a href="#ref-crawfordPointIntervalEstimates2010" role="doc-biblioref">Crawford, Garthwaite, and Porter 2010</a>)</span>. So, <span class="citation">Crawford, Garthwaite, and Porter (<a href="#ref-crawfordPointIntervalEstimates2010" role="doc-biblioref">2010</a>)</span>
proposed the use of <span class="math inline">\(Z\)</span> scores as effect size measures within single-case
neuropsychology and dubbed this measure <span class="math inline">\(Z_{CC}\)</span> where the subscript indicates
“case-controls” comparison:
<span class="math display" id="eq:zcc">\[\begin{equation}
Z_{CC} = \frac{y^* - \overline{y}}{s}.
\tag{2.2}
\end{equation}\]</span>
If <span class="math inline">\(p\)</span> is the percentage of the population that would fall below or above a
case score, <span class="math inline">\(Z_{CC}\)</span> can be used to construct <span class="math inline">\(100(1-\alpha)\)</span>% confidence intervals around <span class="math inline">\(p\)</span>.
The method to do so was described by <span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordInvestigationSingleCase2002" role="doc-biblioref">2002</a>)</span>.</p>
<p>Given <span class="math inline">\(Z_{CC}\)</span> and <span class="math inline">\(y^* \neq \overline{y}\)</span>, <span class="math inline">\(Z_{CC}\)</span> comes from a non-central
<span class="math inline">\(t\)</span> distribution on <span class="math inline">\(n-1\)</span> degrees of freedom. We find the value <span class="math inline">\(\delta_U\)</span> being
the non-centrality parameter (NCP) of a non-central <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(df=n-1\)</span>
such that the <span class="math inline">\(100\frac{\alpha}{2}\)</span> percentile equates <span class="math inline">\(Z_{CC}\sqrt{n}\)</span> and correspondingly
we find the NCP <span class="math inline">\(\delta_L\)</span> of a non-central <span class="math inline">\(t\)</span> distribution such that the <span class="math inline">\(100(1-\frac{\alpha}{2})\)</span>
percentile equates <span class="math inline">\(Z_{CC}\sqrt{n}\)</span>. The easiest approach to finding these quantities
is by deploying a search algorithm. We can then construct the upper and lower boundaries
for <span class="math inline">\(Z_{CC}\)</span> as:
<span class="math display">\[
Z_{CC_U} = \frac{\delta_U}{\sqrt{n}}, \ \ Z_{CC_L} = \frac{\delta_L}{\sqrt{n}}.
\]</span>
When the case falls on the left side of the distribution
the boundaries for <span class="math inline">\(p\)</span> are given by:
<span class="math display">\[
p_U = \Phi\left(\frac{\delta_U}{\sqrt{n}}\right), \ p_L = \Phi\left(\frac{\delta_L}{\sqrt{n}}\right),
\]</span>
where <span class="math inline">\(\Phi\)</span> is the CDF of the standard normal distribution. And
when the case falls on the right side of the distribution the boundaries
are given by:
<span class="math display">\[
p_U = 1 - \Phi\left(\frac{\delta_L}{\sqrt{n}}\right), \ p_L = 1 - \Phi\left(\frac{\delta_U}{\sqrt{n}}\right).
\]</span></p>
<p>Evidently, estimating abnormality on a single variate is trivial. It
is equally simple to estimate abnormality on the difference between two
variates <span class="math inline">\(Y_1 - Y_2\)</span> if the normally distributed variates <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>
do not need standardisation to be comparable. If this is the case then
the test function is similar to (<a href="#eq:TD">(2.1)</a>) but one divides
the differences by the standard deviation of the difference scores:
<span class="math display" id="eq:UDT">\[\begin{equation}
t_{n-1} = \frac{(y^*_1 - \overline{y}_1) - (y^* _2 - \overline{y}_2) }{ \sqrt{(s^2_1 +s^2_2 -2s_1 s_2 \rho_{12})(\frac{n+1}{n})}}
\tag{2.3}
\end{equation}\]</span>
where <span class="math inline">\(\rho_{12}\)</span> is the correlation between the variates. Confidence intervals
for this “unstandardised difference test” (UDT) is constructed as for the TD.
Applications testing unstandardised differences is however scarce, at least
within neuropsychology. Much more common is the need to asses abnormality of
differences between variates that require standardisation to be comparable.</p>
<p>If we just standardise the variates in (<a href="#eq:UDT">(2.3)</a>) and do not
take sample size into account, we get an effect size measure of the
difference (dissociation) similar to <span class="math inline">\(Z_{CC}\)</span>, (<a href="#eq:zcc">(2.2)</a>):
<span class="math display" id="eq:PJ">\[\begin{equation}
Z_{DCC}  = \frac{z^*_1 - z^*_2}{\sqrt{2-2\rho_{12}}}.
\tag{2.4}
\end{equation}\]</span>
The two quantities in the numerator are the standardised case scores on variate
<span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> and the subscript indicates “discrepancy-case-controls”. This
measure was proposed as a reporting standard in the single-case literature by
<span class="citation">Crawford, Garthwaite, and Porter (<a href="#ref-crawfordPointIntervalEstimates2010" role="doc-biblioref">2010</a>)</span>, but was first suggested as a statistic
for significance testing by <span class="citation">Payne and Jones (<a href="#ref-payneStatisticsInvestigationIndividual1957" role="doc-biblioref">1957</a>)</span>.
It is of course not appropriate for such use since we would get an inflated
resultant statistic (<span class="math inline">\(Z_{DCC}\)</span>) because the quantities in the numerator (<span class="math inline">\(z^*_1\)</span> and <span class="math inline">\(z^*_2\)</span>) are inflated.</p>
<p>When we only can estimate standardised case scores from small samples and need
to test if the difference between them is abnormally large we are in fact estimating
the difference between two <span class="math inline">\(t\)</span> distributed variates. Since linear combinations
of correlated <span class="math inline">\(t\)</span> distributed variates are not <span class="math inline">\(t\)</span> distributed themselves this
is not a trivial problem.</p>
<p><span class="citation">Garthwaite and Crawford (<a href="#ref-garthwaiteDistributionDifferenceTwo2004" role="doc-biblioref">2004</a>)</span> examined the distribution of such
difference scores using asymptotic expansion. They searched for quantities
that divided by a function of the sample correlation would be asymptotically
<span class="math inline">\(t\)</span> distributed. They found:</p>
<p><span class="math display">\[\begin{equation}
\psi=\frac{\frac{(y^*_1-\overline{y}_1)}{s_{1}}-\frac{(y^*_2-\overline{y}_2)}{s_{2}}}{
\sqrt{
(\frac{n+1}{n})
\left( (2-2 \rho)+
\frac{2(1-\rho^{2})}{n-1}+
\frac{(5+c^{2})(1-\rho^{2})}{2(n-1)^{2}}+
\frac{\rho(1+c^{2})(1-\rho^{2})}{2(n-1)^{2}}\right)
}},
\end{equation}\]</span></p>
<p><span class="math inline">\(\rho\)</span> being the sample correlation and <span class="math inline">\(c\)</span> a pre-specified critical two-tailed
<span class="math inline">\(t\)</span> value on <span class="math inline">\(df= n-1\)</span>. They showed that <span class="math inline">\(\mathbb{P}[ \psi &gt; c] \approx\mathbb{P}[t &gt;c]\)</span>
and that one must solve for <span class="math inline">\(\psi = c\)</span> to obtain a precise
probability of <span class="math inline">\(\psi\)</span> which yields a quantity that is not dependent on a
specified critical value. This is a quadratic equation in <span class="math inline">\(c^2\)</span>,
choosing the positive root of which yields:</p>
<p><span class="math display" id="eq:RSDT">\[\begin{align}
c &amp; = \sqrt{\frac{ -b + \sqrt{b^2 - 4ad}}{2a}}, \  \text{where} \\
a &amp; = (1+r)(1-r^2), \\
b &amp; =  (1-r)[4(n-1)^2+4(1+r)(n-1)+(1+r)(5+r)], \\
d &amp; =  - 2\left[\frac{y^*_{1} - \overline{y}_1}{s_1}-\frac{y^*_2 -\overline{y}_2}{s_2}\right]^2\left(\frac{n(n-1)^2}{n+1}\right)
\tag{2.5}
\end{align}\]</span></p>
<p>where <span class="math inline">\(p = \mathbb{P}[t_{n-1}&gt;c]\)</span> is used for significance testing and is the
point estimate the percentage of the control population that is expected to show
a more extreme difference score than the case. Note that the test statistic
<span class="math inline">\(c\)</span> will always be positive, no matter the direction of the effect.</p>
<p><span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordTestingSuspectedImpairments2005" role="doc-biblioref">2005</a>)</span> refer to this test statistic as
the “revised standardised difference test” (RSDT) because it was an iteration on
a test previously developed by the authors, that did not keep control of
the Type I error rate <span class="citation">Crawford, Howell, and Garthwaite (<a href="#ref-crawfordPayneJonesRevisited1998" role="doc-biblioref">1998</a>)</span>.
<span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordTestingSuspectedImpairments2005" role="doc-biblioref">2005</a>)</span> show that the RSDT succeeds
in this attempt fairly well.</p>
<p>Using simulations the RSDT was shown to barely exceed the specified error rate
even for very small samples such as <span class="math inline">\(n = 5\)</span>. However, when a case lacks a
dissociation but exhibits extreme scores on both variates (in the same direction
of course) the error rate increases steeply. According to one simulation study
the RSDT starts to lose control of the error rate for scores that are
simultaneously more extreme than two standard deviations away from the mean and
for task scores as extreme as 8 standard deviations away from the mean, error
rates as high as 35% were observed <span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordComparisonSingleCase2007" role="doc-biblioref">2007</a>)</span>.</p>
<p>Another issue with the RSDT is that, because <span class="math inline">\(\psi\)</span> is only approximately
<span class="math inline">\(t\)</span> distributed, constructing confidence intervals for estimated abnormality in
the same manner as for the TD is not possible. So to remedy these drawbacks of this
test statistic Crawford and colleagues looked into Bayesian methodology.</p>
<div id="freqsing" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Example usage in <code>singcar</code></h3>
<p>The test of deficit is called in the <code>singcar</code> package with the function <code>TD()</code>.
Testing for a deficit using the example data from Section <a href="#exdata">2.1.1</a> we have:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">TD</span>(<span class="at">case =</span> caseA, <span class="at">controls =</span> contA, <span class="at">alternative =</span> <span class="st">&quot;less&quot;</span>, <span class="at">conf_level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Test of Deficit
## 
## data:  Case = 0.03, Controls (m = 0.16, sd = 0.08, n = 28)
## t = -1.7243, df = 27, p-value = 0.04804
## alternative hypothesis: true diff. between case and controls is less than 0
## sample estimates:
##   Std. case score (Z-CC), 95% CI [-2.34, -1.15] 
##                                       -1.754857 
## Proportion below case (%), 95% CI [0.95, 12.47] 
##                                        4.804003</code></pre>
<p>Where the argument <code>alternative</code> specifies the alternative hypothesis, i.e.
whether we are performing two or one sided test and if the latter, which direction.
If one instead wants to test for a dissociation the most common alternative hypothesis
is two sided. Using the RSDT for this test, call:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">RSDT</span>(<span class="at">case_a =</span> caseA, <span class="at">case_b =</span> caseB, <span class="at">controls_a =</span> contA, <span class="at">controls_b =</span> contB,</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>      <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Revised Standardised Difference Test
## 
## data:  Case A: 0.03, B: 0.10, Ctrl. A (m, sd): (0.16, 0.08), B: (0.18, 0.10)
## approx. abs. t = 1.015, df = 27, p-value = 0.3191
## alternative hypothesis: true difference between tasks is not equal to 0
## sample estimates:
## Std. case score, task A (Z-CC) Std. case score, task B (Z-CC) 
##                     -1.7548574                     -0.7836956 
##  Std. task discrepancy (Z-DCC)      Proportion below case (%) 
##                     -1.0647889                     15.9560625</code></pre>
<p>Notably, this function does not produce confidence intervals. If the two tasks
are directly comparable without the need of standardisation (which in fact is the
case for this specific dataset), call instead <code>UDT()</code> with the same syntax and
confidence intervals will be given.</p>
</div>
</div>
<div id="sec23" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Bayesian approaches</h2>
<p>The most prominent difference between the frequentist and the Bayesian framework
is that in the former parameters are treated as fixed attributes of a population
and in the latter they are themselves treated as random variables with associated
distributions. To estimate these one can use prior knowledge about the parameter
of interest which is specified as a prior distribution. There is often
a desire that the data should be the only thing influencing estimations. If
so, one can reduce the impact of the prior by, for example, assigning equal
probabilities to all possible parameter values. This is an example of what
is known as a non-informative prior distribution. This distribution is then updated
when new information is obtained from data and as such forms the posterior
parameter distribution. This is calculated using Bayes theorem and if we
omit the normalising constant in the denominator of Bayes theorem it can be
expressed:
<span class="math display">\[\begin{equation*}
posterior \ \propto \ likelihood \times prior.
\end{equation*}\]</span></p>
<p>If we have a hypothesis for a parameter value, what the above is saying is that
the posterior density of this hypothesis is proportional (<span class="math inline">\(\propto\)</span>) to the likelihood
of the data under that hypothesis multiplied by the prior probability
of the hypothesis.</p>
<p>One of the disadvantages with Bayesian methodology is that it might be
impossible or at least very difficult to analytically calculate the posterior.
They are however often feasible to construct with numerical solutions using
Monte Carlo methods. One of the reasons Bayesian statistics has received such
upsurge is because the increase of computational power in recent years that has
made many otherwise infeasible numerical strategies possible. Monte Carlo
methods are mathematical algorithms that solve numerical problems by repeated
random sampling. The algorithms vary depending on the problem at hand, but for
Bayesian methodology they are all building on rules for drawing random values
based on the likelihood and prior – as such “constructing” the posterior with
the draws after a large number of iterations. The peak of the constructed
distribution is often the parameter of interest and the width is a measurement
of the certainty of the estimation. If using non-informative priors the peak
often coincide with the maximum likelihood estimation of the parameter. This
means that non-informative priors often yields estimators with frequentist
properties, necessary for null hypothesis significance testing.</p>
<p>Because the testing property of the estimation methods presented here are of main interest,
frequentist properties was required when <span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordComparisonSingleCase2007" role="doc-biblioref">2007</a>)</span> and
<span class="citation">Crawford, Garthwaite, and Ryan (<a href="#ref-crawfordComparingSingleCase2011" role="doc-biblioref">2011</a>)</span> devised Bayesian analogues to the test functions
presented in Section <a href="#sec22">2.2</a>. The procedural details of these methods
will be described in the following sections. This is to provide details of
how they were implemented in <code>singcar</code> as well as an overview of how they operate.
The methods are all based on Monte Carlo methods and some are, computationally, very
intense. They are implemented in <code>singcar</code> as straight R code but would probably
have benefited from being implemented in compiled code first. This is an aim
for future updates of the package. The following algorithms presented closely
follows the original articles, but with few slight changes of notation to make
the presentation more coherent.</p>
<div id="BTD" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> The Bayesian test of deficit</h3>
<p>The Bayesian analogue of the test of deficit (BTD) produces asymptotically identical
output as the frequentist test of deficit. It produces an estimate of <span class="math inline">\(p\)</span> with accompanying
credible intervals, i.e. the percentage of controls that would exhibit a more extreme
score than the case. The method is included here for completeness but for actual analysis
there is no advantage to using it over the TD, (<a href="#eq:TD">(2.1)</a>).</p>
<p>From a sample of size <span class="math inline">\(n\)</span> we obtain measurements of some normally distributed variate <span class="math inline">\(Y\)</span>
with unknown mean <span class="math inline">\(\mu\)</span> and unknown variance <span class="math inline">\(\sigma^2\)</span>. Let <span class="math inline">\(\overline{y}\)</span> and <span class="math inline">\(s^2\)</span> denote
the sample mean and variance and <span class="math inline">\(y^*\)</span> the case score. The prior distribution of
<span class="math inline">\(\mu\)</span> is conditioned upon the prior distribution of <span class="math inline">\(\sigma^2\)</span> since both parameters
are unknown. The prior considered here is <span class="math inline">\(f(\mu, \sigma^2) \propto (\sigma^2)^{-1}\)</span>,
which is the standard non-informative prior for normal data.
The marginal posterior distribution of <span class="math inline">\(\frac{(n-1)s^2}{\sigma^2} \sim \chi^2_{n-1}\)</span> and
the posterior distribution <span class="math inline">\(\mu|\sigma^2 \sim \mathcal{N}(\overline{y}, \sigma^2/n)\)</span>,
see e.g., <span class="citation">Gelman et al. (<a href="#ref-gelmanBayesianDataAnalysis2013" role="doc-biblioref">2013</a>)</span> (p. 45 and 65).</p>
<p>When we have a posterior distribution for the control population parameters, <span class="math inline">\(p\)</span> can
be estimated by iterating the following steps.</p>
<ol style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\hat{\sigma}^2_{(i)}\)</span> as <span class="math inline">\(\hat{\sigma}^2_{(i)} = \frac{(n-1)s^2}{\psi}\)</span> where
<span class="math inline">\(\psi\)</span> is generated from a <span class="math inline">\(\chi^2\)</span> distribution on <span class="math inline">\(n-1\)</span> degrees of freedom.
The subscript <span class="math inline">\((i)\)</span> indicates that the statistic is an estimate of <span class="math inline">\(\sigma^2\)</span>
for this iteration.</li>
<li>The estimate for <span class="math inline">\(\mu\)</span> for this iteration is given by <span class="math inline">\(\hat{\mu}_{(i)}=\overline{y}+z\sqrt{(\hat{\sigma}_{(i)}^2/n)}\)</span>,
where <span class="math inline">\(z\)</span> is generated from a standard normal distribution.</li>
<li>The estimate of <span class="math inline">\(p\)</span> is then calculated as if <span class="math inline">\(\hat{\sigma}^2_{(i)}\)</span> and
<span class="math inline">\(\hat{\mu}_{(i)}\)</span> are the true population parameters. That is, put
<span class="math inline">\(z^*_{(i)}=\frac{y^* - \hat{\mu}_{(i)}}{\sqrt{\hat{\sigma}_{(i)}^2}}\)</span> as the standardised
case score and obtain an iteration estimate of <span class="math inline">\(p\)</span> as <span class="math inline">\(\hat{p}_{(i)}=\Phi\left(z^*_{(i)}\right)\)</span>
or <span class="math inline">\(\hat{p}_{(i)} = 1-\Phi\left(z^*_{(i)}\right)\)</span> depending on alternative
hypothesis.</li>
</ol>
<p>Iterating these steps a large number of times yields a distribution of <span class="math inline">\(\hat{p}\)</span>,
the mean of which is the point estimate of <span class="math inline">\(p\)</span>, which can be used for significance
testing. The 2.5th and 97.5th percentile of the distribution of <span class="math inline">\(\hat{p}\)</span> as well
as of <span class="math inline">\(z^*\)</span> gives the boundaries for the 95% credible interval of <span class="math inline">\(\hat{p}\)</span> and
<span class="math inline">\(\hat{Z}_{CC}\)</span> respectively. The point estimate of the latter is that of
(<a href="#eq:zcc">(2.2)</a>).</p>
</div>
<div id="BSDT" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> The Bayesian standardised difference test</h3>
<p>In contrast to the BTD, the Bayesian analogue of the difference tests does not
produce asymptotically identical results to the RSDT.<br />
For the Bayesian standardised difference test (BSDT) we now obtain
measurements on <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> following a bivariate normal distribution
representing two tasks of interest, from a control sample of size <span class="math inline">\(n\)</span>. The goal
is to estimate the percentage <span class="math inline">\(p\)</span> of the control population that would be
expected to show a greater difference <span class="math inline">\(Y_1 - Y_2\)</span> than the case.</p>
<p>Let <span class="math inline">\(\overline{y}_1\)</span> and <span class="math inline">\(\overline{y}_2\)</span> be the sample means and<br />
<span class="math display">\[\begin{equation*}
\pmb{A}=\begin{bmatrix}
s^2_{1} &amp; s_{12} \\
s_{12} &amp; s^2_{2} \end{bmatrix}
\end{equation*}\]</span>
the sample variance-covariance matrix.
Then <span class="math inline">\(\pmb{S} =\pmb{A}(n-1)\)</span> is the sums of squares and cross products (SSCP) matrix.
The case scores are denoted <span class="math inline">\(y_1^*\)</span> and <span class="math inline">\(y_2^*\)</span>.</p>
<p>The population parameters
<span class="math display">\[\begin{equation*}
\pmb{\mu} = \begin{pmatrix}
\mu_1 \\
\mu_2 \end{pmatrix} \ \text{and} \ \Sigma=\begin{bmatrix}
\sigma^2_{1} &amp; \sigma_{12} \\
\sigma_{12} &amp; \sigma^2_{2} \end{bmatrix}
\end{equation*}\]</span>
are both unknown. To estimate <span class="math inline">\(p\)</span> we must first obtain a posterior
of these parameters. Since both are unknown the prior distribution
of <span class="math inline">\(\pmb\mu\)</span> is again conditioned upon the prior distribution of <span class="math inline">\(\Sigma\)</span>.</p>
<p>The prior considered for <span class="math inline">\(f(\pmb\mu, \Sigma^{-1})\)</span> in
<span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordComparisonSingleCase2007" role="doc-biblioref">2007</a>)</span> was <span class="math inline">\(f(\pmb\mu, \Sigma^{-1}) \propto |\Sigma|\)</span>. This is a common non-informative prior, however, it was chosen in
favour of the perhaps even more commonly used <span class="math inline">\(f(\pmb\mu, \Sigma^{-1}) \propto |\Sigma|^{(k+1)/2}\)</span> for multivariate normal data, where <span class="math inline">\(k=\)</span> the
number of variates. This was because the former prior was shown to give
asymptotically identical interval estimates as the UDT for unstandardised
case scores. Even though the latter
perhaps is more commonly applied, <span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordComparisonSingleCase2007" role="doc-biblioref">2007</a>)</span> refer to
the former as the “standard theory” prior.</p>
<p>For this prior we have that <span class="math inline">\(\Sigma^{-1} \sim \mathcal{W}_k(\pmb{S}^{-1}, n)\)</span>.
That is, the posterior marginal distribution for <span class="math inline">\(\Sigma^{-1}\)</span> is a Wishart
distribution parametrised with scale matrix <span class="math inline">\(\pmb{S}^{-1}\)</span> and <span class="math inline">\(n\)</span> degrees of
freedom. The Wishart distribution is a multivariate generalisation of the <span class="math inline">\(\chi^2\)</span>
distribution. It is possible to view <span class="math inline">\(S\sim\mathcal{W}_p(\Sigma, n)\)</span> as a distribution
of SSCP-matrices associated with <span class="math inline">\(\Sigma\)</span>. Inversely, one can view the inverse
Wishart distribution parametrised with some SSCP matrix and <span class="math inline">\(n\)</span> degrees of freedom
as a distribution of variance-covariance matrices related to that SSCP matrix.</p>
<p>We then have that <span class="math inline">\(\pmb\mu|\Sigma \sim \mathcal{N}_k(\pmb{\overline{y}}, \Sigma/n)\)</span> where <span class="math inline">\(\mathcal{N}_k(\pmb{\overline{y}}, \Sigma/n)\)</span> denotes a
multivariate normal distribution with mean <span class="math inline">\(\pmb{\overline{y}}\)</span> and variance
<span class="math inline">\(\Sigma/n\)</span>, see e.g., <span class="citation">Gelman et al. (<a href="#ref-gelmanBayesianDataAnalysis2013" role="doc-biblioref">2013</a>)</span> (p. 72-73).</p>
<p>The posterior marginal distribution of <span class="math inline">\(\Sigma\)</span> is then constructed
by iterating draws of random observations from <span class="math inline">\(\mathcal{W}^{-1}(\pmb{S}, n)\)</span>,
each observation, <span class="math inline">\(\hat{\Sigma}_{(i)}\)</span>, being the estimation of <span class="math inline">\(\Sigma\)</span>
for the <span class="math inline">\(i\)</span>th iteration.</p>
<p>As previously mentioned, frequentist properties are desired if the estimations
are to be used for significance testing. <span class="citation">Berger and Sun (<a href="#ref-bergerObjectivePriorsBivariate2008" role="doc-biblioref">2008</a>)</span>
investigated convergence to frequentist estimates for bivariate normal
distributions using various priors. They showed that the “standard theory” prior
produced converging estimates of <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span> but that the
convergence was not as good for <span class="math inline">\(\rho\)</span> and differences in means. As a “general
purpose” prior they instead recommend using <span class="math inline">\(f(\pmb\mu, \Sigma^{-1}) \propto \frac{1}{\sigma_1\sigma_2(1-\rho^2)}\)</span>. Posteriors with this prior is
constructed by using rejection sampling. Random draws from an inverse Wishart on
<span class="math inline">\(n-1\)</span> degrees of freedom are generated, but only a subset of the the
observations are retained.</p>
<p><span class="citation">Crawford, Garthwaite, and Ryan (<a href="#ref-crawfordComparingSingleCase2011" role="doc-biblioref">2011</a>)</span> considered this prior but noticed that it gave
rise to too narrow credible intervals, overestimating the certainty of the estimations.
However, they showed that if the sample was treated as being of size <span class="math inline">\(n-1\)</span> when
estimating <span class="math inline">\(\Sigma\)</span>, i.e. so that the generated intervals were somewhat more conservative,
frequentist properties were retained. This “calibrated” prior is recommended by
<span class="citation">Crawford, Garthwaite, and Ryan (<a href="#ref-crawfordComparingSingleCase2011" role="doc-biblioref">2011</a>)</span>, and they refer to it as such.</p>
<p>To construct the posterior using the calibrated prior the following steps are
iterated:</p>
<ol style="list-style-type: decimal">
<li><p>Since we are reducing the sample size we must put <span class="math inline">\(\pmb{S}^*= (n-2)\pmb{S}/(n-1)\)</span>
to retain the unbiased property of <span class="math inline">\(\pmb{S}/(n-1)\)</span> as an estimate
of <span class="math inline">\(\Sigma\)</span>.</p></li>
<li><p>Draw a random observation from <span class="math inline">\(\mathcal{W}^{-1}(\pmb{S}^*, n-2)\)</span>. This
draw is denoted:
<span class="math display">\[
\hat{\Sigma} = \begin{bmatrix}
\hat{\sigma}^2_{1} &amp; \hat{\sigma}_{12} \\
\hat{\sigma}_{12} &amp; \hat{\sigma}^2_{2} \end{bmatrix}
\]</span>
and
<span class="math display">\[
\hat{\rho}= \frac{\hat{\sigma}_{12}}{\sqrt{\hat{\sigma}^2_{1}\hat{\sigma}^2_{2}}}.
\]</span></p></li>
<li><p>If <span class="math inline">\(u\)</span> is a random draw from a uniform distribution <span class="math inline">\(u \sim U(0, 1)\)</span>, <span class="math inline">\(\hat\Sigma\)</span>
is only accepted as the estimation of <span class="math inline">\(\Sigma\)</span> if <span class="math inline">\(u^2 \leq 1-\hat{\rho}^2\)</span>, if
not we iterate the procedure until we have an accepted draw of <span class="math inline">\(\hat\Sigma\)</span>. Once
accepted we have an estimation of <span class="math inline">\(\Sigma\)</span> for this iteration and denote the draw:
<span class="math display">\[\begin{equation*}
\hat{\Sigma}_{(i)}=\begin{bmatrix}
\hat{\sigma}^2_{1(i)} &amp; \hat{\sigma}_{12(i)} \\
\hat{\sigma}_{12(i)} &amp; \hat{\sigma}^2_{2(i)} \end{bmatrix}.
\end{equation*}\]</span></p></li>
</ol>
<p>Even though this latter calibrated prior is recommended, some might argue that frequentist
properties are of no concern and therefore choose the “standard theory” prior.
But regardless of the prior chosen, when we have an estimate of <span class="math inline">\(\Sigma\)</span>, the following
steps are iterated to obtain an estimate of <span class="math inline">\(p\)</span>, the percentage of controls expected
to exhibit a more extreme difference between the variates than the case.</p>
<ol style="list-style-type: decimal">
<li><p>Generate two draws from a standard normal distribution and denote them <span class="math inline">\(z_{r1}\)</span> and <span class="math inline">\(z_{r2}\)</span>.
Find the lower triangular matrix <span class="math inline">\(\pmb{T}\)</span> such that <span class="math inline">\(\pmb{T}\pmb{T}&#39; = \hat\Sigma_{(i)}\)</span> using
Choleksy decomposition. The estimate of <span class="math inline">\(\pmb{\mu}\)</span> for this iteration is then:
<span class="math display">\[\begin{equation*}
\pmb{\hat{\mu}}_{(i)} = \begin{pmatrix}
\hat{\mu}_{1(i)} \\
\hat{\mu}_{2(i)} \end{pmatrix} = \begin{pmatrix}
\overline{y}_1 \\
\overline{y}_2 \end{pmatrix}+ \pmb{T} \begin{pmatrix}
z_{r1} \\
z_{r2} \end{pmatrix} / \sqrt{n}.
\end{equation*}\]</span>
See for example <span class="citation">Gelman et al. (<a href="#ref-gelmanBayesianDataAnalysis2013" role="doc-biblioref">2013</a>)</span> (p. 580).</p></li>
<li><p>We now have estimates of both <span class="math inline">\(\pmb{\mu}\)</span> and <span class="math inline">\(\Sigma\)</span> and calculate
<span class="math inline">\(p\)</span> as if these were the true population parameters. The method to do so
differ slightly depending on whether a standardised or unstandardised test
is required. For an unstandardised test, the size of the difference score is calculated
as:<br />
<span class="math display">\[\begin{equation*}
z_{(i)}^* = \frac{(y_1^* - \hat{\mu}_{1(i)}) - (y^*_2 - \hat{\mu}_{2(i)})}
{\sqrt{\hat{\sigma}^2_{1(i)}+\hat{\sigma}^2_{2(i)}-2\hat{\sigma}_{12(i)}}}.
\end{equation*}\]</span>
More commonly we would want to calculate the difference score from standardised
variates. If this is the case, put:
<span class="math display">\[\begin{equation*}
z_{1(i)} = \frac{y_1^* - \hat{\mu}_{1(i)}}{\sqrt{\hat{\sigma}^2_{1(i)}}}, \ z_{2(i)} = \frac{y_2^* -
\hat{\mu}_{2(i)}}{\sqrt{\hat{\sigma}^2_{2(i)}}}, \ \hat{\rho}_{(i)} = \frac{\hat{\sigma}_{12(i)}}{\sqrt{\hat{\sigma}_{1(i)}\hat{\sigma}_{2(i)}}}
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
\hat{z}^*_{(i)} = \frac{z_{1(i)} - z_{2(i)}}{\sqrt{2-2\hat{\rho}_{(i)}}}.
\end{equation*}\]</span></p></li>
<li><p>The estimate of <span class="math inline">\(p\)</span>, <span class="math inline">\(\hat{p}_{(i)}\)</span>, for this iteration is then the tail area of a standard
normal distribution more extreme than <span class="math inline">\(z^*_{(i)}\)</span>.</p></li>
</ol>
<p>Iterate the procedure a large number of times and save the estimations for each
iteration. This will yield a distribution of <span class="math inline">\(\hat{p}\)</span> and <span class="math inline">\(\hat{z}^*\)</span>, the
quantiles of which again are used to calculate the boundaries of the credible
intervals for <span class="math inline">\(p\)</span> and <span class="math inline">\(Z_{DCC}\)</span>. The point estimate of the former is the mean of
the estimated distribution, while the point estimate of the latter is that of
(<a href="#eq:PJ">(2.4)</a>).</p>
</div>
<div id="bayesing" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Example usage in <code>singcar</code></h3>
<p>Testing for a deficit using the Bayesian test of deficit, call <code>BTD()</code>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">BTD</span>(<span class="at">case =</span> caseA, <span class="at">controls =</span> contA, <span class="at">alternative =</span> <span class="st">&quot;less&quot;</span>,</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">iter =</span> <span class="dv">10000</span>, <span class="at">int_level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Bayesian Test of Deficit
## 
## data:  Case = 0.03, Controls (m = 0.16, sd = 0.08, n = 28)
## df = 27, p-value = 0.04799
## alternative hypothesis: true diff. between case and controls is less than 0
## sample estimates:
##   Std. case score (Z-CC), 95% CI [-2.33, -1.16] 
##                                       -1.754857 
## Proportion below case (%), 95% CI [0.98, 12.38] 
##                                        4.799212</code></pre>
<p>As can be seen, compared to <code>TD()</code> this function has the additional argument <code>iter</code>, which
indicates the number of iterations to be used for the posterior approximation.
This number should be based on desired accuracy of the analysis.</p>
<p>Testing for a dissociation using the BSDT (recommended over RSDT), call:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">BSDT</span>(<span class="at">case_a =</span> caseA, <span class="at">case_b =</span> caseB, <span class="at">controls_a =</span> contA, <span class="at">controls_b =</span> contB,</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>      <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>, <span class="at">iter =</span> <span class="dv">10000</span>, <span class="at">unstandardised =</span> <span class="cn">FALSE</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>      <span class="at">calibrated =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
##  Bayesian Standardised Difference Test
## 
## data:  Case A: 0.03, B: 0.10, Ctrl. A (m, sd): (0.16,0.08), B: (0.18,0.10)
## df = 26, p-value = 0.3212
## alternative hypothesis: true difference between tasks is not equal to 0
## sample estimates:
##                  Std. case score, task A (Z-CC) 
##                                      -1.7548574 
##                  Std. case score, task B (Z-CC) 
##                                      -0.7836956 
## Std. discrepancy (Z-DCC), 95% CI [-1.69, -0.40] 
##                                      -1.0647889 
## Proportion below case (%), 95% CI [4.53, 34.43] 
##                                      16.0584420</code></pre>
<p>This function has several additional arguments compared to <code>RSDT()</code>.
Instead of having a separate function for an unstandardised
Bayesian difference test one can specify whether to standardise
the variates or not within the function. If <code>unstandardised</code> is
set to <code>TRUE</code>, the results from <code>BSDT()</code> will converge to that of
<code>UDT()</code>. To use the “standard theory” prior instead of the calibrated,
set <code>calibrated</code> to <code>FALSE</code>.</p>
</div>
<div id="cov" class="section level3" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Tests using Bayesian regression to allow for covariates</h3>
<p>It is seldom possible to design perfect experiments. This is especially true in
psychological science where causal relationships can be confounded by a great
variety of factors. When comparing a case to a control sample it is therefore
important to reduce noise by matching the control group to the case as well as
possible on variates beyond the ones of interest, such as age or education
level, making the collection of control samples cumbersome.</p>
<p>However, this matching of covariates can instead be achieved statistically.
<span class="citation">Crawford, Garthwaite, and Ryan (<a href="#ref-crawfordComparingSingleCase2011" role="doc-biblioref">2011</a>)</span> describe methods where they extend the
tests presented in <a href="#sec22">2.2</a> to allow for the inclusion of covariates
using Bayesian regression. These methods thus allow you to compare the case’s
score on the task(s) of interest against the control population conditioned
on them having the same score as the case on the covariate.
This both reduces noise and alleviate the collection of control data.
But because one degree of freedom is lost for each included covariate
it is advisable to only include covariates with a correlation to the variate(s)
of interest larger than <span class="math inline">\(0.3\)</span>, as a rule of thumb <span class="citation">(<a href="#ref-crawfordComparingSingleCase2011" role="doc-biblioref">Crawford, Garthwaite, and Ryan 2011</a>)</span>.</p>
<p>We assume that the variates of interest are normally distributed, but no assumption
of the distribution of the covariates is made. The procedure to get an estimate of
<span class="math inline">\(p\)</span> in the presence of covariates is presented below for the general case, that is
either when we are interested in a deficit or a dissociation. The main difference
between the methods is the recommended prior.</p>
<p>From a sample of size <span class="math inline">\(n\)</span> we obtain measurements on <span class="math inline">\(k= 1,2\)</span> variates of interest
an <span class="math inline">\(m\)</span> number of covariates, denoted <span class="math inline">\(\boldsymbol{X} = (X_1, \dots, X_m)\)</span>.</p>
<p>We wish to estimate the regression coefficients for each covariate on the variate(s)
of interest:
<span class="math display">\[
\boldsymbol{B} = \begin{bmatrix}
\boldsymbol{\beta}_1 &amp; \cdots &amp; \boldsymbol{\beta}_k
\end{bmatrix}.
\]</span>
Here <span class="math inline">\(\boldsymbol{B}\)</span> is an <span class="math inline">\(m+1 \times1,2\)</span> matrix where each column contains
the regression coefficients for the <span class="math inline">\(i\)</span>th variate of interest, the first
row being the intercept(s). Further, we wish to estimate the covariance matrix
or variance for the variate(s) of interest conditioned upon the covariates, we
assume these statistics not to vary with the covariates.
<span class="math display">\[
\Sigma \ | \ \boldsymbol{X} =
\begin{bmatrix}
\sigma^2_1 \ | \ \boldsymbol{X} &amp; \rho\sigma_1\sigma_2 \ | \ \boldsymbol{X} \\
\rho\sigma_1\sigma_2 \ | \ \boldsymbol{X} &amp; \sigma^2_2 \ | \ \boldsymbol{X}
\end{bmatrix}  \ \text{or} \ \left[\sigma^2 | \boldsymbol{X} \right].
\]</span></p>
<p>If <span class="math inline">\(\boldsymbol{X}\)</span> is the <span class="math inline">\(n \times (m+1)\)</span> design matrix
and <span class="math inline">\(\boldsymbol{Y}\)</span>, the <span class="math inline">\(n \times k\)</span> response matrix
<span class="math display">\[
\boldsymbol{X} =
\begin{bmatrix}
1 &amp; x_{11} &amp; \cdots &amp; x_{1m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n1} &amp; \cdots &amp; x_{nm}
\end{bmatrix},\ \
\boldsymbol{Y} =
\begin{bmatrix}
y_{11} &amp; \cdots &amp; y_{1k} \\
\vdots &amp; \ddots &amp; \vdots \\
y_{n1} &amp; \cdots &amp; y_{nk}
\end{bmatrix}
\]</span>
the data estimates of <span class="math inline">\(\boldsymbol{B}\)</span> and <span class="math inline">\(\Sigma\)</span> are
<span class="math display">\[
\boldsymbol{B}^* = (\boldsymbol{X}&#39;\boldsymbol{X})^{-1}\boldsymbol{X}&#39;\boldsymbol{Y} \ \text{and} \
\Sigma^* = \frac{1}{n-m-1}(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{B}^*)&#39;(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{B}^*).
\]</span></p>
<p>When <span class="math inline">\(k=1\)</span> or the “standard theory” prior is used for <span class="math inline">\(k=2\)</span> the posterior
of <span class="math inline">\(\Sigma\)</span> is an inverse Wishart with scale matrix <span class="math inline">\((n-m-1)\Sigma^*\)</span>
on <span class="math inline">\(n-m-1\)</span> or <span class="math inline">\(n-m\)</span> degrees of freedom for <span class="math inline">\(k=1\)</span> and <span class="math inline">\(k=2\)</span> respectively
<span class="citation">(<a href="#ref-crawfordComparingSingleCase2011" role="doc-biblioref">Crawford, Garthwaite, and Ryan 2011</a>; <a href="#ref-tiaoBayesianEstimationMultivariate1964" role="doc-biblioref">Tiao and Zellner 1964</a>)</span>.
Hence, to get an estimate <span class="math inline">\(\hat{\Sigma}_{(i)}\)</span> of <span class="math inline">\(\Sigma\)</span> we generate
a draw from these distributions.</p>
<p>For the “calibrated” prior we instead follow the procedure outlined in Section
<a href="#BSDT">2.3.2</a> and use <span class="math inline">\(\pmb{S}^* = (n-m-2)\pmb{S}/(n-m-1)\)</span> as scale
matrix to sample from an inverse Wishart on <span class="math inline">\(n-m-2\)</span> degrees of freedom,
<span class="math inline">\(\pmb{S} = (n-m-1)\Sigma^*\)</span>. The estimates are then:</p>
<p><span class="math display" id="eq:sigma" id="eq:sigma">\[\begin{equation}
\hat{\Sigma}_{(i)}=[\hat{s}^2_{(i)}] \ \text{when} \ k=1 \ \text{and} \
\hat{\Sigma}_{(i)} =
\begin{bmatrix}
\hat{s}^2_{1(i)} &amp; \hat{s}_{12(i)}  \\
\hat{s}_{12(i)}  &amp; \hat{s}^2_{2(i)}
\end{bmatrix} \ \text{when} \ k=2. \
\tag{2.6}
\end{equation}\]</span>   k=2.<br />
\tag{2.6}
\end{equation}</p>
<p>We turn the <span class="math inline">\((m+1) \times k\)</span> matrix <span class="math inline">\(\boldsymbol{B}^*\)</span> into a <span class="math inline">\(k(m + 1) \times 1\)</span> vector by concatenating the columns in <span class="math inline">\(\boldsymbol{B}^*\)</span>.
<span class="math inline">\(\boldsymbol{B}^*_{vec} = (\boldsymbol{\beta}^{*&#39;}_1, ...,\boldsymbol{\beta}^{*&#39;}_k)&#39;\)</span> is the estimate of <span class="math inline">\(\boldsymbol{B}_{vec}\)</span>.
The posterior of <span class="math inline">\(\boldsymbol{B}_{vec}| \hat{\Sigma}_{(i)}\)</span> is a multivariate
normal distribution with mean <span class="math inline">\(\boldsymbol{B}^*_{vec}\)</span> and covariance
matrix <span class="math inline">\(\boldsymbol{\Lambda}_{(i)} =\boldsymbol{\hat\Sigma}_{(i)} \otimes (\boldsymbol{X&#39;X})^{-1}\)</span>,
where <span class="math inline">\(\otimes\)</span> denotes the Kronecker product. To get an estimate of <span class="math inline">\(\hat{\boldsymbol{B}}_{(i)}\)</span>
we generate a random observation from this distribution where the <span class="math inline">\(k(m+1) \times 1\)</span> vector
of obtained values
<span class="math inline">\(\hat{\boldsymbol{B}}^*_{\text{vec(i)}} =(\hat{\boldsymbol{\beta}}&#39;_{1(i)}, ...,\hat{\boldsymbol{\beta}}&#39;_{k(i)})&#39;\)</span>
is turned back into the <span class="math inline">\(m+1 \times k\)</span> matrix
<span class="math inline">\(\hat{\boldsymbol{B}}_{(i)} = (\hat{\boldsymbol{\beta}}_{1(i)}, ...,\hat{\boldsymbol{\beta}}_{k(i)})\)</span>.
If <span class="math inline">\(\boldsymbol{x}^*\)</span> is a vector of the case values on the covariates we obtain
estimates of the conditional case scores on the task of interest as so:
<span class="math display">\[
\hat{\boldsymbol{\mu}}_{(i)} = \hat{\boldsymbol{B}}_{(i)}\boldsymbol{x}^*.
\]</span></p>
<p>With these values we can now estimate the conditional effect size of the case.
<span class="citation">Crawford, Garthwaite, and Ryan (<a href="#ref-crawfordComparingSingleCase2011" role="doc-biblioref">2011</a>)</span> denote these effect measures <span class="math inline">\(Z_{CCC}\)</span> and
<span class="math inline">\(Z_{DCCC}\)</span> depending on whether <span class="math inline">\(k=1\)</span> or <span class="math inline">\(k=2\)</span>. As the names indicate they are
related to <span class="math inline">\(Z_{CC}\)</span> (<a href="#eq:zcc">(2.2)</a>) and <span class="math inline">\(Z_{DCC}\)</span> (<a href="#eq:PJ">(2.4)</a>),
but calculated from statistics conditioned upon the covariates, hence the extra
C in the subscript.</p>
<p>If <span class="math inline">\(y^*_1\)</span> is the case score on a variate <span class="math inline">\(Y_1\)</span>,
we would calculate the conditional deficit (<span class="math inline">\(Z_{DCCC}\)</span>) of the case on variate <span class="math inline">\(Y_1\)</span> like so:
<span class="math display" id="eq:zccc">\[\begin{equation}
\hat{Z}_{CCC(i)} = \frac{y^*_1-\hat{\mu}_{(i)}}{\hat{s}^2_{(i)}}.
\tag{2.7}
\end{equation}\]</span>
If <span class="math inline">\(y^*_2\)</span> is the case score on variate <span class="math inline">\(Y_2\)</span> and we are interested
in a dissociation between <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>, we denote the two
conditional means on these variates <span class="math inline">\(\hat{\mu}_{1(i)}\)</span> and <span class="math inline">\(\hat{\mu}_{2(i)}\)</span>.
<span class="math inline">\(Z_{DCCC}\)</span> is then calculated like so:
<span class="math display" id="eq:zdccc">\[\begin{equation}
\hat{Z}_{DCCC(i)} = \frac{\frac{y^*_1-\hat{\mu}_{1(i)}}{\hat{s}_{1(i)}}-\frac{y^*_2-\hat{\mu}_{2(i)}}
{\hat{s}_{2(i)}}}{\sqrt{2-2\hat{\rho}_{12(i)}}}.
\tag{2.8}
\end{equation}\]</span>
The conditional standard deviations <span class="math inline">\(\hat{s}_{1(i)}\)</span> and <span class="math inline">\(\hat{s}_{2(i)}\)</span>
are obtained from <span class="math inline">\(\hat{\Sigma}_{(i)}\)</span> in (<a href="#eq:sigma">(2.6)</a>), the
conditional correlation between the variates (<span class="math inline">\(\hat{\rho}_{12(i)}\)</span>) is given by
<span class="math display">\[\hat{\rho}_{12(i)} = \frac{\hat{s}_{12(i)}}{\sqrt{\hat{s}^2_{1(i)}\hat{s}^2_{2(i)}}}.\]</span></p>
<p>To get estimates of <span class="math inline">\(p\)</span> we find the tail area under the standard normal distribution
with more extreme values on <span class="math inline">\(\hat{Z}_{CCC(i)}\)</span> or <span class="math inline">\(\hat{Z}_{DCCC(i)}\)</span>, depending
on the problem and alternative hypothesis specified. If testing for a defict we would
for example have:
<span class="math display">\[
\hat{p}_{(i)}= \Phi(\hat{Z}_{CCC(i)}).
\]</span></p>
<p>Iterate the procedure a large number of times and save the estimations for each
iteration. The mean of the resultant distribution of <span class="math inline">\(\hat{p}\)</span> is taken at the point
estimate of <span class="math inline">\(p\)</span>. Point estimates of <span class="math inline">\(Z_{CCC}\)</span> and <span class="math inline">\(Z_{DCCC}\)</span> are obtained by
using (<a href="#eq:zccc">(2.7)</a>) and (<a href="#eq:zdccc">(2.8)</a>) with the conditional means,
standard deviations and correlation calculated directly from the control sample.
The <span class="math inline">\(1-\alpha\)</span> credible interval boundaries for each estimate is obtained from the
quantiles of the estimated distributions as described for the previous methods.</p>
</div>
<div id="covsing" class="section level3" number="2.3.5">
<h3><span class="header-section-number">2.3.5</span> Example usage in <code>singcar</code></h3>
<p>To specify a Bayesian test of deficit with one or more covariates, call:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">BTD_cov</span>(<span class="at">case_task =</span> caseA, <span class="at">case_covar =</span> caseAGE, <span class="at">control_task =</span> contA,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">control_covar =</span> contAGE, <span class="at">alternative =</span> <span class="st">&quot;less&quot;</span>, <span class="at">int_level =</span> <span class="fl">0.95</span>,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">iter =</span> <span class="dv">10000</span>, <span class="at">use_sumstats =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## 
##  Bayesian Test of Deficit with Covariates
## 
## data:  Case = 0.03, Controls (m = 0.16, sd = 0.08, n = 28)
## df = 26, p-value = 0.05201
## alternative hypothesis: true diff. between case and controls is less than 0
## sample estimates:
##  Std. case score (Z-CCC), 95% CI [-2.30, -1.10] 
##                                       -1.749556 
## Proportion below case (%), 95% CI [1.08, 13.60] 
##                                        5.200838</code></pre>
<p>The covariates should for the case be specified as a single value or
vector of values containing the case scores for each covariate included.
For the controls the covariates should be specified as a vector or matrix
where each column represents one of the covariates.</p>
<p>To specify a Bayesian test for a dissociation with one or more covariates present, call:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">BSDT_cov</span>(<span class="at">case_tasks =</span> <span class="fu">c</span>(caseA, caseB), <span class="at">case_covar =</span> caseAGE, </span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>          <span class="at">control_tasks =</span> <span class="fu">cbind</span>(contA, contB), <span class="at">control_covar =</span> contAGE, </span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>          <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>, <span class="at">int_level =</span> <span class="fl">0.95</span>, <span class="at">iter =</span> <span class="dv">10000</span>,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">calibrated =</span> <span class="cn">TRUE</span>, <span class="at">use_sumstats =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## 
##  Bayesian Standardised Difference Test with Covariates
## 
## data:  Case A = 0.03, B = 0.10, Ctrl. (m, sd) A: (0.16,0.08), B: (0.18,0.10)
## df = 25, p-value = 0.3299
## alternative hypothesis: true difference between tasks is not equal to 0
## sample estimates:
##                   Std. case score, task A (Z-CC) 
##                                        -1.754857 
##                   Std. case score, task B (Z-CC) 
##                                        -0.783696 
## Std. discrepancy (Z-DCCC), 95% CI [-1.66, -0.41] 
##                                        -1.064152 
##  Proportion below case (%), 95% CI [4.84, 34.10] 
##                                        16.500000</code></pre>
<p>Here, the case scores as well as the control scores
from variates of interest must be concatenated. The covariates are specified
in the same manner as for <code>BTD_cov()</code>. The prior used is again specified with
the argument <code>calibrated</code>.</p>
<p>If using summary statistics for these functions, the argument <code>use_sumstats</code>
must be set to <code>TRUE</code>. Information on how to specify the data in such a case
consult the documentation.</p>
</div>
</div>
<div id="lmm" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Using mixed effects models for repeated measures</h2>
<p>It should be noted that the TD of course is nothing other than a linear
regression model with a dummy coded variable for belonging to the patient group.
This equivalence might seem trivial but is included here for completeness. If
<span class="math inline">\(\boldsymbol{y} = (y^*_1, y_2,...,y_n)\)</span> is the vector of scores for the case <span class="math inline">\(y^*_1\)</span> and
the control sample <span class="math inline">\((y_2,...,y_n)\)</span> on the variate of interest, then <span class="math inline">\(\boldsymbol{x} = (1, 0, ..., 0)\)</span> is a dummy coded variable indicating inclusion in the patient
group. The equation for a regression model with these variables is then:
<span class="math display">\[
y_i = \beta_0+\beta_1x_i+\epsilon_i,
\]</span>
where the error term <span class="math inline">\(\epsilon_i\sim\mathcal{N}(0, \sigma^2)\)</span>. The least squares
estimate of <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\hat{\beta}_1\)</span> equals the numerator in the equation for
the TD, (<a href="#eq:TD">(2.1)</a>). We have:
<span class="math display">\[\begin{equation}
\hat{\beta}_1 &amp; = \frac{\frac{1}{n}\sum^n_{i =1}(x_i-\overline{x})(y_i-\overline{y})}{\frac{1}{n}\sum^n_{i = 1}(x_i-\overline{x})^2} =\frac{\sum^n_{i =1}y_ix_i-\frac{1}{n}\sum^n_{i =1}y_i\sum^n_{i =1}x_i}{\sum^n_{i=1}(x_i-\overline{x})^2} \\
&amp; =\frac{\frac{ny^*_1-\sum^n_{i=1}y_i}{n}}{\left(1-\frac{1}{n}\right)^2+(n-1)\left(-\frac{1}{n}\right)^2}
=\frac{\frac{ny^*_1-(y^*_1+\sum^n_{i=2}y_i)}{n}}{\frac{n-1}{n}}
=\frac{(n-1)y^*_1-\sum^n_{i=2}y_i}{n-1} \\
&amp; =\frac{(n-1)y^*_1-\sum^n_{i=2}y_i}{n-1}
=y^*_1-\frac{\sum^n_{i=2}y_i}{n-1}
=y^*_1-\overline{y}_c.
\end{equation}\]</span>
Here <span class="math inline">\(\overline{y}_c\)</span> denotes the sample mean of the controls, which is
also the least squares estimate of the intercept, <span class="math inline">\(\hat{\beta}_0\)</span>:
<span class="math display">\[\begin{equation}
\hat{\beta}_0 &amp;= \overline{y}-\hat{\beta}_1\overline{x} = \frac{\sum^n_{i=1}y_i}{n} -\frac{ny_1-\sum^n_{i=1}y_i}{(n-1)}\frac{1}{n} \\
&amp;= \frac{(n-1)\sum^n_{i=1}y_i-ny_1+\sum^n_{i=1}y_i}{n(n-1)}\\
&amp;= \frac{n\sum^n_{i=1}y_i-ny_1}{n(n-1)}
= \frac{\sum^n_{i=2}y_i}{n-1} = \overline{y}_c.
\end{equation}\]</span></p>
<p>To test the slope in a simple regression model one uses the statistic
<span class="math display" id="eq:regt">\[\begin{equation}
t_{n-2} = \frac{\hat{\beta}_1}{SE\left(\hat{\beta}_1\right)}
\tag{2.9}
\end{equation}\]</span>
following a <span class="math inline">\(t\)</span> distribution on <span class="math inline">\(n-2\)</span> degrees of freedom, given that the null
hypothesis is true. The degrees of freedom for this distribution is of course the
same as <span class="math inline">\(n_c-1\)</span> degrees of freedom if <span class="math inline">\(n_c\)</span> is the size of the control sample.
Remember that the denominator in the TD was
<span class="math inline">\(s_c \sqrt{\frac{n_c + 1}{n_c}}\)</span>, where <span class="math inline">\(s_c\)</span> is the standard deviation of the control
sample. We know that the standard error for the slope coefficient
is:
<span class="math display">\[
SE\left(\hat{\beta}_1\right) = \sqrt{\frac{\frac{1}{n-2}\sum^n_{i =1}\hat{\epsilon}^2_i}{\sum^2_{i=1}(x_i-\overline{x})^2} }
= \sqrt{\frac{\frac{1}{n-2}\sum^n_{i=1}(y_i-\hat{y}_i)^2}{\frac{n-1}{n}}}.
\]</span>
Furthermore, <span class="math inline">\(\hat{y}_i = \hat{\beta}_0+\hat{\beta}_1x_i\)</span> and
<span class="math inline">\(\hat{y}_1 = \overline{y}_c+(y^*_1-\overline{y}_c) = y^*_1\)</span>,
hence:
<span class="math display" id="eq:regsd">\[\begin{equation}
SE\left(\hat{\beta}_1\right)
&amp;= \sqrt{\frac{\frac{1}{n-2}\sum^n_{i=2}(y_i-\hat{y}_i)^2+\cancel{(y^*_1-\hat{y}_1)^2}}{\frac{n-1}{n}}} \\
&amp;= \sqrt{\frac{\sum^n_{i=2}(y_i-\hat{y}_i)^2}{n-2}}\sqrt{\frac{n}{n-1}} = s_c\sqrt{\frac{n_c+1}{n_c}}.
\tag{2.10}.
\end{equation}\]</span>
As such the <span class="math inline">\(t\)</span> test for the slope in the regression equation and the test of
deficit are identical. To get <span class="math inline">\(Z_{CC}\)</span> if using the regression approach simply
divide <span class="math inline">\(\hat{\beta}_1\)</span> by the standard deviation of the sample,
<span class="math inline">\(Z_{CC} = \hat{\beta}_1/s_c\)</span>.</p>
<p>The fact that we can use simple linear regression instead of the TD to model
case abnormality is in itself not very interesting. However, one can
include covariates without going through the trouble of performing Bayesian
regression for the methods as described in Section <a href="#cov">2.3.4</a>. To compute the
conditional effect size <span class="math inline">\(Z_{CCC}\)</span> in this case one must first calculate the
conditional mean and standard deviation from the control sample. The conditional
mean can be predicted from the fitted model given the case values on the
covariates and the conditional variance can be computed like so:
<span class="math display">\[
\sigma^2|X = \frac{\sum^n_{i=2}\epsilon_i^2}{n-2},
\]</span>
where <span class="math inline">\(X\)</span> denotes the covariates and <span class="math inline">\(\epsilon_i\)</span> the residuals from the fitted
model. Note also that the index starts at 2 to leave out the case. <span class="math inline">\(Z_{CCC}\)</span> is then:
<span class="math display">\[
Z_{CCC} = \frac{y_1^*-\mu|X}{\sqrt{\sigma^2|X}}.
\]</span></p>
<p>More importantly though, we can generalise the linear regression to
linear mixed effect models <span class="citation">(<a href="#ref-west2014linear" role="doc-biblioref">West, Welch, and Galecki 2014</a>)</span>. With linear
mixed effect models (LMM) one can model data with dependency between
observations, opening for the possibility to use repeated measures data in
single case comparisons and other, more complex, designs. This is because
LMM can model both fixed and random effects (hence “mixed”). Fixed effects are
in this case fixed or constant across participants and random effects are
found for observations that are somehow more related to each other (e.g. 
several measurements from the same participant or hierarchically clustered
observations). The concept of applying LMM to single case comparisons was
first investigated by <span class="citation">Huber et al. (<a href="#ref-huberComparingSingleCase2015a" role="doc-biblioref">2015</a>)</span>.</p>
<p>Since impairments on tasks converging on a cognitive construct of interest
provides stronger evidence of a potential deficit, the possibility to model
random effects successfully is of great importance. Such a model would take the
form:
<span class="math display" id="eq:lmm1">\[\begin{equation}
y_{ij} = \beta_0 + \beta_1x_i + v_{0i}+\epsilon_{ij}, \  v_{0i} \sim \mathcal{N}(0, \sigma^2_v) \ \text{and} \ \epsilon_{ij} \sim \mathcal{N}(0, \sigma^2).
\tag{2.11}
\end{equation}\]</span>
Here <span class="math inline">\(i\)</span> indicates participant and <span class="math inline">\(j\)</span> indicates task or item. So, <span class="math inline">\(y_{ij}\)</span> is
then the response of participant <span class="math inline">\(i\)</span> for task/item <span class="math inline">\(j\)</span>. <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are
still the intercept of the sample and raw effect of the case while <span class="math inline">\(v_{0i}\)</span>
denotes the random intercept for each participant. As such the term allows for
participants to vary from the average intercept of the sample. However, while
the model in (<a href="#eq:lmm1">(2.11)</a>) accounts for participant variability we
must include yet a further term to account for the task/item variability:
<span class="math display" id="eq:lmm2">\[\begin{equation}
y_{ij} = \beta_0 + \beta_1x_i + v_{0i} + w_{0j} +\epsilon_{ij}, \\
v_{0i} \sim \mathcal{N}(0, \sigma^2_v), \ w_{0j} \sim \mathcal{N}(0, \sigma^2_w) \ \text{and} \ \epsilon_{ij} \sim \mathcal{N}(0, \sigma^2),
\tag{2.12}
\end{equation}\]</span>
where <span class="math inline">\(w_{0j}\)</span> denotes the random intercept for tasks/items, which should be included
to avoid inflation of Type I errors <span class="citation">(<a href="#ref-baayenMixedeffectsModelingCrossed2008" role="doc-biblioref">Baayen, Davidson, and Bates 2008</a>)</span>.</p>
<p>A common method to obtain <span class="math inline">\(p\)</span> values for LMMs is to use likelihood ratio tests.
However, <span class="citation">Huber et al. (<a href="#ref-huberComparingSingleCase2015a" role="doc-biblioref">2015</a>)</span> show that the Type I error rate for this
method only is at an acceptable level when the size of the control sample is
larger than 50. In contrast, using parametric bootstrapping or one-tailed <span class="math inline">\(t\)</span>
tests approximating the degrees of freedom with the Satterthwaite method, kept
the error rate at its nominal level (or rather somewhat above, implying that a
slightly more conservative <span class="math inline">\(\alpha\)</span> value should be considered) even for small
control sample sizes. As a reference they compared error rate and statistical
power of the mixed model to the TD using aggregated data, i.e. taking the mean
of all tasks/items for each participant, which is what <span class="citation">Crawford and Howell (<a href="#ref-crawfordComparingIndividualTest1998" role="doc-biblioref">1998</a>)</span>
suggests for this type of data. The <span class="math inline">\(p\)</span>
value of <span class="math inline">\(\beta_1\)</span> in (<a href="#eq:lmm2">(2.12)</a>) using the Satterthwaite method
will be approximately equal to the two sided <span class="math inline">\(p\)</span> value from the TD if averaging the scores for
each participant over the tasks/items, given a large enough sample size.</p>
<p>Furthermore, because we can model data with dependency between observations
with LMM it is also possible to model effects within participants, i.e.
between conditions. The model equation would then be:
<span class="math display" id="eq:lmm3">\[\begin{equation}
y_{ij} = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i}+ v_{0i} + w_{0j} +\epsilon_{ij}, \\
v_{0i} \sim \mathcal{N}(0, \sigma^2_v), \ w_{0j} \sim \mathcal{N}(0, \sigma^2_w) \ \text{and} \ \epsilon_{ij} \sim \mathcal{N}(0, \sigma^2),
\tag{2.13}
\end{equation}\]</span>
where <span class="math inline">\(\beta_2\)</span> is the raw effect of the condition indicated by the categorical
variable <span class="math inline">\(x_2\)</span>. Introducing an interaction term between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> in this model
we can even test for the presence of a dissociation. This model would take the form:
<span class="math display" id="eq:lmm4">\[\begin{equation}
y_{ij} = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i}+\beta_3(x_{1i}x_{2i})+ v_{0i} + v_{1i} + w_{0j} +\epsilon_{ij}, \\
\begin{pmatrix}
v_{0i} \\
v_{1i}
\end{pmatrix}
  \sim \mathcal{N}\left(
  \begin{bmatrix}
0 \\
0
\end{bmatrix},
\Sigma_v =
\begin{bmatrix}
\sigma^2_{v_0} &amp;  \sigma_{v_0}\sigma_{v_1} \\
\sigma_{v_0}\sigma_{v_1} &amp; \sigma^2_{v_1}
\end{bmatrix} \right), \ w_{0j} \sim \mathcal{N}(0, \sigma^2_w) \ \text{and} \ \epsilon_{ij} \sim \mathcal{N}(0, \sigma^2),
\tag{2.14}
\end{equation}\]</span>
where <span class="math inline">\(\beta_3\)</span> is the raw effect of the dissociation and <span class="math inline">\(v_{1i}\)</span> is
the random slope of the participants. That is, <span class="math inline">\(v_{1i}\)</span> accounts for
the variability of the effect of the condition, between participants.
A caveat of using LMM is that it requires more observations
than parameters specified, to be identifiable. Specifying random slopes
and random intercepts for each participant then one must have <span class="math inline">\(&gt;2n\)</span> observations.
Implying that it would not be a good model for testing a dissociation
between two tasks without repeated measurements within each task.</p>
<p>There are already several implementations of LMMs in various open source
software which can be used directly in the context of single case comparisons
as described above. Therefore this methodology has not been implemented in
<code>singcar</code>. However, due to the usefulness of the concept and the lack
of methodology in <code>singcar</code> to handle repeated measures data I will give
a brief description of how one can specify an LMM in <code>R</code> using the package
<code>lme4</code> <span class="citation">(<a href="#ref-batesFittingLinearMixedeffects2015" role="doc-biblioref">Bates et al. 2015</a>)</span> and
<code>lmerTest</code> <span class="citation">(<a href="#ref-kuznetsovaLmerTestPackageTests2017" role="doc-biblioref">Kuznetsova, Brockhoff, and Christensen 2017</a>)</span> (the latter is used to
obtain <span class="math inline">\(p\)</span> values from <span class="math inline">\(t\)</span> tests with Satterthwaite approximation of
the degrees of freedom).</p>
<div id="lmmexample" class="section level3" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Example usage in <code>R</code></h3>
<p>Begin by installing and loading required packages:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">c</span>(<span class="st">&quot;lme4&quot;</span>, <span class="st">&quot;lmerTest&quot;</span>, <span class="st">&quot;MASS&quot;</span>))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;lme4&quot;</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;lmerTest&quot;</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;MASS&quot;</span>) <span class="co"># For multivariate simulation</span></span></code></pre></div>
<p>Because the example data is not suited for repeated measures analysis
a simulated dataset will be used. Say that we have 30 measurements from
four normally distributed variates <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(i = 1,2,3,4\)</span>. Call these variates items.
They are thought to converge on some cognitive ability of interest and have the distribution:
<span class="math display">\[
\begin{pmatrix}
Y_1 \\
Y_2 \\
Y_3 \\
Y_4
\end{pmatrix} \sim \mathcal{N}\left(
\begin{bmatrix}
100 \\
80 \\
50 \\
20
\end{bmatrix},
\begin{bmatrix}
15^2 &amp; 108 &amp; 78 &amp; 30 \\
108 &amp; 10^2 &amp; 12 &amp; 15 \\
78 &amp; 12 &amp; 10^2 &amp; 25 \\
30 &amp; 15 &amp; 25 &amp; 5^2
\end{bmatrix}
\right).
\]</span>
The case has incurred a deficit of two standard deviations on all four variates.
These data could then be simulated as follows:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>simdata <span class="ot">&lt;-</span>  <span class="fu">expand.grid</span>(<span class="at">case =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">29</span>)), <span class="at">item =</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>simdata<span class="sc">$</span>ID <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">30</span>, <span class="dv">4</span>)) </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># For replicability</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">80</span>, <span class="dv">50</span>, <span class="dv">20</span>)  </span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">15</span><span class="sc">^</span><span class="dv">2</span>, <span class="dv">108</span>, <span class="dv">78</span>, <span class="dv">30</span>,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                  <span class="dv">108</span>, <span class="dv">10</span><span class="sc">^</span><span class="dv">2</span>, <span class="dv">12</span>, <span class="dv">15</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>                  <span class="dv">78</span>,  <span class="dv">12</span>, <span class="dv">10</span><span class="sc">^</span><span class="dv">2</span>, <span class="dv">25</span>,</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>                  <span class="dv">30</span>,  <span class="dv">15</span>,  <span class="dv">25</span>,  <span class="dv">5</span><span class="sc">^</span><span class="dv">2</span>), <span class="at">nrow =</span> <span class="dv">4</span>, <span class="at">byrow =</span> T)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>dv <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="dv">30</span>, <span class="at">mu =</span> mu, <span class="at">Sigma =</span> sigma) <span class="co"># Dependent variable</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>dv[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> dv[<span class="dv">1</span>, ] <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">30</span>, <span class="sc">-</span><span class="dv">20</span>, <span class="sc">-</span><span class="dv">20</span>,  <span class="sc">-</span><span class="dv">10</span>) <span class="co"># Case scores</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>simdata<span class="sc">$</span>dv <span class="ot">&lt;-</span> <span class="fu">c</span>(dv)</span></code></pre></div>
<p>The linear mixed model to test the deficit of the case would then
be</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lmer</span>(dv <span class="sc">~</span> case <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>ID) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>item), <span class="at">data =</span> simdata)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1)[[<span class="st">&quot;coefficients&quot;</span>]]</span></code></pre></div>
<pre><code>##              Estimate Std. Error        df   t value    Pr(&gt;|t|)
## (Intercept)  62.15587  17.512303  3.030078  3.549269 0.037506018
## case        -25.07172   7.793911 27.999869 -3.216834 0.003263006</code></pre>
<p>Where <code>(1|ID)</code> and <code>(1|item)</code> specifies the random effects structure,
in this case the random intercepts for participant and item.
This is approximately the same <span class="math inline">\(t\)</span> statistic that would be produced
by the test of deficit if averaging the scores for each participant:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>agg_data <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(dv)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">TD</span>(agg_data[<span class="dv">1</span>], agg_data[<span class="sc">-</span><span class="dv">1</span>], <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>)[[<span class="st">&quot;statistic&quot;</span>]]</span></code></pre></div>
<pre><code>##        t 
## -3.21684</code></pre>
<p>Now, say that we want to model a dissociation using LMM. We have
collected 30 measurements from two items during two different
conditions. It is thought that these two different conditions
are tapping two independent cognitive abilities but that the
items collected in each condition are converging on the same.
Hence we have four variates
<span class="math inline">\(Y_{ij}\)</span>, <span class="math inline">\(i=1,2\)</span>, <span class="math inline">\(j=1,2\)</span>, distributed according to the multivariate
distribution:
<span class="math display" id="eq:lmmdist">\[\begin{equation}
\begin{pmatrix}
Y_{11} \\
Y_{21} \\
Y_{12} \\
Y_{22}
\end{pmatrix} \sim \mathcal{N}\left(
\begin{bmatrix}
100 \\
80 \\
50 \\
20
\end{bmatrix},
\begin{bmatrix}
15^2 &amp; 120 &amp; 45 &amp; 7 \\
120 &amp; 10^2 &amp; 11 &amp; 15 \\
45 &amp; 11 &amp; 10^2 &amp; 40 \\
7 &amp; 15 &amp; 40 &amp; 5^2
\end{bmatrix}
\right),
\tag{2.15}
\end{equation}\]</span>
where e.g. <span class="math inline">\(Y_{12}\)</span> is the random variable
corresponding to the first item in the second condition.
Given that the two conditions in fact taps different cognitive
structures, the incurred impairment of the case would potentially
affect performance in one of the conditions more than the other.
Hence, we impose a deficit of two standard deviations on the case scores.
The data can then be simulated as follows:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>simdata <span class="ot">&lt;-</span>  <span class="fu">expand.grid</span>(<span class="at">case =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">29</span>)),</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">item =</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>), <span class="at">condition =</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>))</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>simdata<span class="sc">$</span>ID <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">30</span>, <span class="dv">4</span>)) </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(simdata<span class="sc">$</span>condition) <span class="ot">&lt;-</span> <span class="fu">contr.sum</span>(<span class="dv">2</span>) <span class="co"># Effect coding</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># For replicability</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">80</span>, <span class="dv">50</span>, <span class="dv">20</span>)  </span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">15</span><span class="sc">^</span><span class="dv">2</span>, <span class="dv">120</span>,    <span class="dv">45</span>,  <span class="dv">7</span>,</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>                  <span class="dv">120</span>, <span class="dv">10</span><span class="sc">^</span><span class="dv">2</span>,    <span class="dv">11</span>, <span class="dv">15</span>,</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>                  <span class="dv">45</span>,    <span class="dv">11</span>,  <span class="dv">10</span><span class="sc">^</span><span class="dv">2</span>, <span class="dv">40</span>,</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>                  <span class="dv">7</span>,     <span class="dv">15</span>,    <span class="dv">40</span>, <span class="dv">5</span><span class="sc">^</span><span class="dv">2</span>), <span class="at">nrow =</span> <span class="dv">4</span>, <span class="at">byrow =</span> T)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>dv <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="dv">30</span>, <span class="at">mu =</span> mu, <span class="at">Sigma =</span> sigma) <span class="co"># Dependent variable</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>dv[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> dv[<span class="dv">1</span>, ] <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">30</span>, <span class="sc">-</span><span class="dv">20</span>, <span class="sc">-</span><span class="dv">0</span>,  <span class="sc">-</span><span class="dv">0</span>) <span class="co"># Case scores</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>simdata<span class="sc">$</span>dv <span class="ot">&lt;-</span> <span class="fu">c</span>(dv)</span></code></pre></div>
<p>The fully crossed linear mixed model used to test a dissociation for these
data would then be specified like so:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lmer</span>(dv <span class="sc">~</span> case<span class="sc">*</span>condition <span class="sc">+</span> (condition<span class="sc">|</span>ID) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span> item), <span class="at">data =</span> simdata)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">summary</span>(model2)[[<span class="st">&quot;coefficients&quot;</span>]], <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##                  Estimate Std. Error       df  t value Pr(&gt;|t|)
## (Intercept)      61.87447   12.18101  1.02152  5.07959  0.11994
## case            -17.24035    7.66811 27.99986 -2.24832  0.03261
## condition1       28.08855    0.98493 28.00058 28.51834  0.00000
## case:condition1 -13.82757    5.39468 28.00058 -2.56319  0.01603</code></pre>
<p>Comparing the <span class="math inline">\(p\)</span> value of the interaction effect to the <span class="math inline">\(p\)</span> value
obtained from <code>RSDT()</code> we see that that they are far from equivalent.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>agg_data1 <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(dv[ , <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>agg_data2 <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(dv[ , <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">RSDT</span>(agg_data1[<span class="dv">1</span>], agg_data2[<span class="dv">1</span>], agg_data1[<span class="sc">-</span><span class="dv">1</span>], agg_data2[<span class="sc">-</span><span class="dv">1</span>])[[<span class="st">&quot;p.value&quot;</span>]]</span></code></pre></div>
<pre><code>## [1] 0.06737687</code></pre>
<p>However, a small simulation study conducted with the data described above
revealed that the RSDT and the LMM yields comparable results regarding power and
Type I error. It takes the specified
impairments of the case for each variate and returns the ratio
of significant interaction effects from (<a href="#eq:lmm4">(2.14)</a>) divided
by the total number of simulations run, as well as the same ratio
obtained from either the RSDT or the BSDT. The distribution
of the data is the same in this small simulation study as (<a href="#eq:lmmdist">(2.15)</a>).</p>
<pre><code>R&gt; powerLMM(nsim = 1000, case_impairment = c(-30, -20, -0, -0),
+ compare_against = &quot;RSDT&quot;, alpha = 0.05)
RSDT LMM
1 0.353 0.462</code></pre>
<p>It seems like the method based on the linear mixed model has quite a
good power advantage over the RSDT for data simulated from this
particular distribution. To compare the error rate of two tests
when can use the same ratio as for power, when there is no true
effect present. In this case that would be either when the case
has no incurred impairment at all or when the impairments are
equal in both conditions.</p>
<pre><code>R&gt; powerLMM(nsim = 1000, case_impairment = c(-0, -0, -0, -0),
+ compare_against = &quot;RSDT&quot;, alpha = 0.05)
RSDT LMM
1 0.032 0.039</code></pre>
<p>Even though a power advantage of the mixed model was observed it seems to
produce an error rate under the nominal level of <span class="math inline">\(0.05\)</span> as well. If this was a
consistent behaviour it would eliminate the usefulness of the RSDT. However, if
the case exhibits extreme impairments in both conditions we would have another
null scenario. This was discussed previously in Section <a href="#sec22">2.2</a> where it
was noted that one of the reasons <span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordComparisonSingleCase2007" role="doc-biblioref">2007</a>)</span> developed a
Bayesian approach to test for dissociations was the highly increasing error rate
of the RSDT, for increasing deficits on both tasks with no discrepancy between
them. Say that the case has impairments of six standard deviations on both items
in both conditions and let us compare the LMM to the BSDT instead of the RSDT:</p>
<pre><code>R&gt; powerLMM(nsim = 1000, case_impairment = c(-90, -60, -60, -30),
+ compare_against = &quot;BSDT&quot;, alpha = 0.05)
BSDT LMM
1 0.053 0.61</code></pre>
<p>The BSDT seems to keep the error rate almost at its nominal level while it is
enormous for the LMM. Even though a more extensive simulation study with
various types of data structures should be performed to solidify these
findings, the preliminary results shown here indicate some limitations
of using LMM for dissociations and reiterates the recommendation of
<span class="citation">Crawford and Garthwaite (<a href="#ref-crawfordComparisonSingleCase2007" role="doc-biblioref">2007</a>)</span> to use the BSDT to test for
dissociations, even when using aggregated repeated measures data.
That is, because the effects of brain damage can be severe I
would recommend against using LMM to model dissociations.</p>
</div>
</div>
<div id="section4" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Multivariate methods</h2>
<p>Up until now I have discussed univariate testing methods. However,
as discussed in the introduction, Section <a href="#section1">1</a>, stronger
evidence for either deficits or dissociations is provided by using
multiple measures (tasks) for a cognitive function of interest.
Conceptually, what we want to estimate or test in this scenario
is the distance between a vector of observations for a single case
and a vector of population means. A common way to do this is by
using the Mahalanobis distance measure. If <span class="math inline">\(\boldsymbol{y}^*\)</span> is a vector
of task observations from a single case and <span class="math inline">\(\boldsymbol{\mu}\)</span> is the population
mean vector, the Mahalanobis distance, <span class="math inline">\(\delta\)</span>, is a measure
of the distance between the case and the population mean, given by:
<span class="math display">\[
\delta= \sqrt{(\boldsymbol{y}^*-\boldsymbol{\mu})&#39;\Sigma^{-1}(\boldsymbol{y}^*-\boldsymbol{\mu})}.
\]</span></p>
<p>Given that the population follows a multivariate normal distribution
we have that <span class="math inline">\(\delta^2 \sim \chi^2_{\nu_1}\)</span> where <span class="math inline">\(\nu_1\)</span> is the degrees
of freedom and the number of dimensions in the multivariate distribution. This
squared distance is sometimes referred to as the Mahalanobis <em>index</em>.
Because the purpose of case-comparison methodology is to estimate
abnormality we are interested in <span class="math inline">\(p\)</span>, that is the proportion of the control
population that would exhibit larger <span class="math inline">\(\delta\)</span> than a single case. If we
need to estimate <span class="math inline">\(\boldsymbol{\mu}\)</span> and <span class="math inline">\(\Sigma\)</span> the sample
Mahalanobis distance is given by
<span class="math display">\[
\hat\delta = \sqrt{(\boldsymbol{y}^*-\overline{\boldsymbol{y}})&#39;\boldsymbol{S}^{-1}(\boldsymbol{y}^*-\overline{\boldsymbol{y}})},
\]</span>
where <span class="math inline">\(\overline{\boldsymbol{y}}\)</span> is the sample mean vector and <span class="math inline">\(\boldsymbol{S}\)</span> the sample
covariance matrix. Using the Mahalanobis index between a case and the mean of
a normative sample, the abnormality is estimated with
<span class="math display">\[
\hat{p} = \mathbb{P}[(\boldsymbol{y}-\overline{\boldsymbol{y}})&#39;\boldsymbol{S}^{-1}(\boldsymbol{y}-\overline{\boldsymbol{y}}) &gt; (\boldsymbol{y}^*-\overline{\boldsymbol{y}})&#39;\boldsymbol{S}^{-1}(\boldsymbol{y}^*-\overline{\boldsymbol{y}})].
\]</span></p>
<p>An obvious way to estimate <span class="math inline">\(p\)</span> would be to use the Hotelling’s <span class="math inline">\(T^2\)</span> test.
The <span class="math inline">\(T^2\)</span> statistic is proportional to the <span class="math inline">\(F\)</span> distribution and hence its
related <span class="math inline">\(p\)</span> value seems like an intuitive choice as an estimate of <span class="math inline">\(p\)</span>.
This was in fact proposed by <span class="citation">Huizenga et al. (<a href="#ref-huizengaMultivariateNormativeComparisons2007" role="doc-biblioref">2007</a>)</span>,
but <span class="citation">Elfadaly, Garthwaite, and Crawford (<a href="#ref-elfadalyPointEstimationAbnormality2016" role="doc-biblioref">2016</a>)</span> show that this estimate
is biased and devise instead several other candidates. None of these was shown to
be perfectly unbiased but several of the estimates performed better than
the estimate based on the <span class="math inline">\(T^2\)</span> statistic.</p>
<p>If we let <span class="math inline">\(\lambda = \delta^2 = (\boldsymbol{y}^*-\boldsymbol{\mu})&#39;\Sigma^{-1}(\boldsymbol{y}^*-\boldsymbol{\mu})\)</span>
and <span class="math inline">\(\chi^2_{\nu_1}= (\boldsymbol{y}-\boldsymbol{\mu})&#39;\Sigma^{-1}(\boldsymbol{y}-\boldsymbol{\mu})\)</span> be the case’s
Mahalanobis index and the Mahalanobis index of any vector <span class="math inline">\(\boldsymbol{x}\)</span> respectively,
what we want to estimate is:
<span class="math display">\[
p = \mathbb{P}[\chi^2_{\nu_1}&gt; \lambda].
\]</span>
If <span class="math inline">\(\overline{\boldsymbol{y}}\)</span> and
<span class="math inline">\(\hat\Sigma = \frac{n-1}{n}\boldsymbol{S}\)</span> denotes the maximum likelihood estimates of <span class="math inline">\(\boldsymbol{\mu}\)</span> and
<span class="math inline">\(\Sigma\)</span> the Hotelling’s <span class="math inline">\(T^2\)</span> based estimate of <span class="math inline">\(p\)</span>, <span class="math inline">\(\hat{p}_F\)</span>, is
<span class="math display">\[
\hat{p}_F = \mathbb{P}\left[(\boldsymbol{y}-\overline{\boldsymbol{y}})&#39;\hat\Sigma^{-1}(\boldsymbol{y}-\overline{\boldsymbol{y}})&gt;(\boldsymbol{y}^*-\overline{\boldsymbol{y}})&#39;\hat\Sigma^{-1}(\boldsymbol{y}^*-\overline{\boldsymbol{y}})\right].
\]</span>
If we let <span class="math inline">\(\lambda_0 = (\boldsymbol{y}^*-\overline{\boldsymbol{y}})&#39;\boldsymbol{S}^{-1}(\boldsymbol{y}^*-\overline{\boldsymbol{y}})\)</span>, we
have that
<span class="math display">\[
T^2=\frac{n\lambda_0}{n+1} \ \text{and} \ \hat{p}_F = \mathbb{P}\left[F_{\nu_1, \nu_2}&gt;\frac{\nu_2}{\nu_1(n-1)}T^2\right].
\]</span>
i.e., this estimate is the <span class="math inline">\(p\)</span> value associated with testing
the hypothesis that the case is coming from the control population,
using a Hotelling’s <span class="math inline">\(T^2\)</span> test. As such it is also often used as
a point estimate of <span class="math inline">\(p\)</span>.
However, we can also use the fact that <span class="math inline">\(\lambda \sim \chi^2_{\nu_1}\)</span>
and use the <span class="math inline">\(p\)</span> value from the <span class="math inline">\(\chi^2\)</span> distribution by only using
the sample estimates on the right hand side of the inequality:
<span class="math display">\[
\hat{p}_{\chi^2} = \mathbb{P}\left[(\boldsymbol{y}-\boldsymbol{\mu})&#39;\Sigma^{-1}(\boldsymbol{y}-\boldsymbol{\mu})&gt;(\boldsymbol{y}^*-\overline{\boldsymbol{y}})&#39;\hat\Sigma^{-1}(\boldsymbol{y}^*-\overline{\boldsymbol{y}})\right].
\]</span>
We have that:
<span class="math display">\[
\hat{p}_{\chi^2} = \mathbb{P}\left[\chi^2_{\nu_1}&gt;\frac{n}{n-1}\lambda_0\right].
\]</span>
This is an intuitive estimator and it was shown to exhibit fairly low bias
(lower than <span class="math inline">\(\hat{p}_F\)</span>) for low true values of <span class="math inline">\(p\)</span> and it had the smallest mean
square error of all estimator considered by
<span class="citation">Elfadaly, Garthwaite, and Crawford (<a href="#ref-elfadalyPointEstimationAbnormality2016" role="doc-biblioref">2016</a>)</span>. But unfortunately, as <span class="math inline">\(p\)</span> increases
<span class="math inline">\(\hat{p}_{\chi^2}\)</span> starts to show substantial bias.</p>
<p>Both of theses estimates are intuitive, however, since they both are biased
<span class="citation">Elfadaly, Garthwaite, and Crawford (<a href="#ref-elfadalyPointEstimationAbnormality2016" role="doc-biblioref">2016</a>)</span> recommend using other estimators, based
on the needs of the analysis. One of which is based on the a modified confidence
interval for Mahalanobis distance and two are based on quadrature polynomial approximation of
the <span class="math inline">\(\chi^2\)</span> distribution. In Section <a href="#pd">2.5.1</a> I describe two estimators
based on confidence intervals of <span class="math inline">\(\delta\)</span> which are both implemented in <code>singcar</code>
along with <span class="math inline">\(\hat{p}_F\)</span> and <span class="math inline">\(\hat{p}_{\chi^2}\)</span>.</p>
<div id="pd" class="section level3" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Estimation based on confidence intervals</h3>
<p><span class="citation">Reiser (<a href="#ref-reiserConfidenceIntervalsMahalanobis2001" role="doc-biblioref">2001</a>)</span> suggested a method to construct
confidence intervals on <span class="math inline">\(\delta^2\)</span>. Using this method <span class="citation">Elfadaly, Garthwaite, and Crawford (<a href="#ref-elfadalyPointEstimationAbnormality2016" role="doc-biblioref">2016</a>)</span>
propose a way to construct confidence intervals on <span class="math inline">\(p\)</span> as well. We have:
<span class="math display">\[
F_0 = \frac{n\nu_2}{(n-1)\nu_1}\lambda_0 \sim F_{\nu_1, \nu_2}(n\lambda),
\]</span>
that is, the sample statistic <span class="math inline">\(F_0\)</span> has a non-central <span class="math inline">\(F\)</span> distribution
on <span class="math inline">\(\nu_1\)</span> and <span class="math inline">\(\nu_2\)</span> degrees of freedom and non-centrality parameter
<span class="math inline">\(n\lambda\)</span>. We call the value of <span class="math inline">\(n\lambda\)</span>, where <span class="math inline">\(F_0\)</span> is the <span class="math inline">\(\alpha\)</span>-quantile
of this distribution, <span class="math inline">\(L_{\alpha}\)</span>. We can construct the lower and upper boundaries
of a confidence interval for <span class="math inline">\(p\)</span> using the CDF for the <span class="math inline">\(\chi^2\)</span>-distribution on <span class="math inline">\(\nu_1\)</span> degrees of
freedom, denoted <span class="math inline">\(G(.)\)</span>, as such:
<span class="math display" id="eq:DCI">\[\begin{equation}
\left[1-G\left(\frac{L_{\alpha/2}}{n}\right), \ 1-G\left(\frac{L_{1-\alpha/2}}{n}\right)\right].
\tag{2.16}
\end{equation}\]</span></p>
<p>Similarly, they suggest a point estimator of <span class="math inline">\(p\)</span> based on the median of
<span class="math inline">\(F_{\nu_1, \nu_2}(n\lambda)\)</span>. So that if we let <span class="math inline">\(L_{0.5}\)</span> be the value of
<span class="math inline">\(n\lambda\)</span> where <span class="math inline">\(F_0\)</span> is the median of <span class="math inline">\(F_{\nu_1, \nu_2}(n\lambda)\)</span> the
estimator is:
<span class="math display" id="eq:pmed">\[\begin{equation}
\hat{p}_D= 1-G\left(\frac{L_{0.5}}{n}\right).
\tag{2.17}
\end{equation}\]</span></p>
<p>Similarly we would construct a confidence interval around <span class="math inline">\(\lambda\)</span> by
choosing <span class="math inline">\(\lambda_u\)</span> and <span class="math inline">\(\lambda_l\)</span> such that
<!-- However, the method to construct confidence intervals around $\delta$ by -->
<!-- @reiserConfidenceIntervalsMahalanobis2001 has a drawback. The intervals are -->
<!-- constructed by exploiting the result that if $\boldsymbol{Y} \sim -->
<!-- \mathcal{N}(\boldsymbol{\mu}, \ \Sigma)$ and $\boldsymbol{M} \sim \mathcal{W}(\Sigma,\ n-1)$ -->
<!-- then $\frac{\nu_2}{\nu_1}\boldsymbol{Y}'\boldsymbol{M}^{-1}\boldsymbol{Y} \sim F_{\nu_1,n- -->
<!-- \nu_1}(\boldsymbol{\mu}'\Sigma\boldsymbol{\mu})$, i.e. a non-central F-distribution with -->
<!-- NCP $\boldsymbol{\mu}'\Sigma\boldsymbol{\mu}$. From this we have: (## Make this stringent with the paragraphs above) -->
<!-- $$ -->
<!-- D^2 = \frac{n\nu_2}{(n-1)\nu_1}\hat{\delta}^2 \sim F_{\nu_1, \nu_2}\left(n\delta^2\right) -->
<!-- $$ -->
<!-- and the upper and lower boundaries of $\delta$, $\delta_u$ and $\delta_l$ -->
<!-- are chosen such that -->
<span class="math display" id="eq:freqci">\[\begin{equation}
\mathbb{P}\left[F_{\nu_1,n- \nu_1}(n\lambda_l)\leq F_0\right] = \frac{\alpha}{2}, \ \text{and} \  \mathbb{P}\left[F_{\nu_1,n- \nu_1}(n\lambda_u)\leq F_0\right] =1- \frac{\alpha}{2}.
\tag{2.18}
\end{equation}\]</span></p>
<p>However, this method has a drawback. It exactly matches
the nominal level, i.e. all of the say 95% intervals (assuming <span class="math inline">\(\alpha= 0.05\)</span>) will in fact contain the
true value of <span class="math inline">\(\delta\)</span> 95% of the time, proved by
<span class="citation">Wunderlich et al. (<a href="#ref-wunderlichExactConfidenceIntervals2015" role="doc-biblioref">2015</a>)</span>. But when <span class="math inline">\(F_0\)</span> is small enough so that
<span class="math inline">\(\mathbb{P}[F_{\nu_1,n- \nu_1}(0)\leq F_0] \leq 1- \frac{\alpha}{2}\)</span>, we cannot
find a <span class="math inline">\(\lambda_u\)</span> so that <span class="math inline">\(\mathbb{P}\left[F_{\nu_1,n- \nu_1}(n\lambda_u)\leq F_0\right] =1- \frac{\alpha}{2}\)</span>
and it is instead set to 0. Similarly we set <span class="math inline">\(\lambda_l\)</span> to 0 when <span class="math inline">\(\mathbb{P}[F_{\nu_1,n- \nu_1}(0)\leq F_0] \leq \frac{\alpha}{2}\)</span>. But we know with full certainty that an interval of [0, 0]
will not contain <span class="math inline">\(\lambda\)</span>, since <span class="math inline">\(\boldsymbol{y}^*\)</span> is continuous with <span class="math inline">\(\mathbb{P}[\boldsymbol{y}^* = \boldsymbol{\mu}] = 0\)</span>.</p>
<p>This problem follows in the construction of confidence intervals for <span class="math inline">\(p\)</span>,
(<a href="#eq:DCI">(2.16)</a>) as well as for the estimator <span class="math inline">\(\hat{p}_D\)</span>, (<a href="#eq:pmed">(2.17)</a>).
The median of the distribution <span class="math inline">\(F_{\nu_1, \nu_2}(n\lambda)\)</span>
decreases of course as the non-centrality parameter <span class="math inline">\(n\lambda\)</span> decreases. We
have that <span class="math inline">\(\lambda \geq 0\)</span>, therefore the lower limit of the median for
<span class="math inline">\(F_{\nu_1,\nu_2}(n\lambda)\)</span> is the median of of this distribution when
the non-centrality parameter <span class="math inline">\(n\lambda = 0\)</span>, i.e. the median of a <em>central</em>
<span class="math inline">\(F\)</span> distribution on <span class="math inline">\(\nu_1\)</span> and <span class="math inline">\(\nu_2\)</span> degrees of freedom. So, using the
approach of <span class="citation">Reiser (<a href="#ref-reiserConfidenceIntervalsMahalanobis2001" role="doc-biblioref">2001</a>)</span>, if <span class="math inline">\(F_0\)</span> is
smaller than this limit one set <span class="math inline">\(n\lambda\)</span> to zero.</p>
<p>We see, however, a bias in this
estimator when <span class="math inline">\(F_0\)</span> is small even if it is not smaller than the lower limit.
<span class="citation">Elfadaly, Garthwaite, and Crawford (<a href="#ref-elfadalyPointEstimationAbnormality2016" role="doc-biblioref">2016</a>)</span> gives as an example of this scenario where
we have <span class="math inline">\(\nu_1 =4, \ \nu_2 = 20\)</span> and <span class="math inline">\(\lambda_0 = 0.4\)</span>, hence <span class="math inline">\(n =24\)</span> and
we have <span class="math inline">\(F_0 = \frac{24* 20}{23 * 4}0.4 = 2.09\)</span>. The value of the non-centrality
parameter <span class="math inline">\(n\lambda\)</span> where the median of <span class="math inline">\(F_{\nu_1, \nu_2}(n\lambda)\)</span> equals 2.09
is ~5.015 and <span class="math inline">\(\hat{p}_D = 1-G_{4}\left(5.015/24\right) = 0.9949\)</span> or 99.49%.
So when using the estimated Mahalanobis index of <span class="math inline">\(0.4\)</span> we would infer that only
0.51% of the population would exhibit a lower Mahalanobis index. If instead
calculating the true percentage of the population that would exhibit a smaller
Mahalanobis index when 0.4 is the true case value, that is <span class="math inline">\(p = 1-G_4(0.4) = 0.9825\)</span>
or 98.25%, which is quite a considerable discrepancy.</p>
<p><span class="citation">Garthwaite, Elfadaly, and Crawford (<a href="#ref-garthwaiteModifiedConfidenceIntervals2017" role="doc-biblioref">2017</a>)</span> proposed a solution to this in the
construction of confidence intervals of <span class="math inline">\(\delta\)</span> by modifying the method by
<span class="citation">Reiser (<a href="#ref-reiserConfidenceIntervalsMahalanobis2001" role="doc-biblioref">2001</a>)</span>. They suggested to form a Bayesian
credible interval instead of the frequentist confidence interval, whenever
<span class="math inline">\(F_0\)</span> is small. Credible intervals have the more intuitive interpretation
that they <em>contain</em> the true value of interest with a probability of
<span class="math inline">\(1-\alpha\)</span>. As such this interval will not give rise to unreasonable values
such as [0, 0]. But to form a credible interval, a prior must be specified.</p>
<p>Let <span class="math inline">\(\boldsymbol{y}\)</span> be a vector of values from an observation of the control population
and not necessarily the case. We denote the prior for <span class="math inline">\(\delta\)</span> when <span class="math inline">\(\delta\)</span>
is the Mahalanobis distance of the case as <span class="math inline">\(\pi(\delta)\)</span> and when <span class="math inline">\(\delta\)</span>
is the distance between the mean of the controls and a randomly selected observation
as <span class="math inline">\(\psi(\delta)\)</span>. A priori <span class="math inline">\(\delta\)</span> should be larger when it is calculated from
the observation of the case than a randomly selected observation of the controls.
<span class="citation">Garthwaite, Elfadaly, and Crawford (<a href="#ref-garthwaiteModifiedConfidenceIntervals2017" role="doc-biblioref">2017</a>)</span> assume that for any <span class="math inline">\(\hat{\delta}^2\)</span>
and any specified quantile <span class="math inline">\(\pi_q(\delta | \hat{\delta}) \geq \psi_q(\delta | \hat{\delta})\)</span>,
where <span class="math inline">\(\pi_q(.)\)</span> and <span class="math inline">\(\psi_q(.)\)</span> are the q:th quantile of the prior distributions.</p>
<p>Note that they do not aim to specify <span class="math inline">\(\pi(\delta)\)</span>, instead they want to put limits on
the quantiles of the resultant posterior through <span class="math inline">\(\psi(\delta)\)</span>. This is because we
know neither <span class="math inline">\(\pi(\delta)\)</span> nor <span class="math inline">\(\pi_q(\delta | \hat{\delta})\)</span> but it is possible to
establish <span class="math inline">\(\psi(\delta|\hat{\delta})\)</span> and the <span class="math inline">\(\alpha/2\)</span> and <span class="math inline">\(1-\alpha/2\)</span> quantiles
of this distribution are then the upper and lower boundaries of a <span class="math inline">\(1-\alpha\)</span> credible interval
of <span class="math inline">\(\delta\)</span>.</p>
<p>The logic behind this modification is intuitive: treat the case like a randomly drawn observation
from the control distribution whenever the case observation is more similar to the average
of the controls than the majority of the control observations. That is, we take the boundaries
that are lowest of the confidence and credible intervals.</p>
<p>Again, we set <span class="math inline">\(\lambda = \delta^2\)</span> and <span class="math inline">\(\hat{\lambda} = \hat{\delta}^2\)</span> and let
<span class="math inline">\(\psi^*(\lambda)\)</span> be the prior corresponding to <span class="math inline">\(\psi(\delta)\)</span>. This is a <span class="math inline">\(\chi^2\)</span>
distribution on <span class="math inline">\(\nu_1\)</span> degrees of freedom.
<span class="math display" id="eq:prior">\[\begin{equation}
\psi^*(\lambda) = \frac{1}{2^{\nu_1/2}\Gamma(\nu_1/2)}\lambda^{\nu_1/2-1}e^{-\lambda/2}, \ \lambda \geq 0.
\tag{2.19}
\end{equation}\]</span>
If we set <span class="math inline">\(m = n/(n-1)\)</span> the likelihood of <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(L(\lambda; \hat{\lambda})\)</span>, is given by
<span class="math display" id="eq:likeli">\[\begin{equation}
L(\lambda; \hat{\lambda}) = \sum_{r=0}^{\infty}\frac{(n\lambda/2)^re^{-n\lambda}}{B[(n-\nu_1)/2, \ \nu_1/2+r]r!}m^{\nu_1/2+r}\left(\frac{1}{1+m\hat{\lambda}}\right)^{n/2+r}\hat{\lambda}^{\nu_1/2+r-1}, \  \lambda \geq 0.
\tag{2.20}
\end{equation}\]</span>
Combining the prior with the likelihood, <span class="math inline">\(\psi^*(\lambda)L(\lambda; \hat{\lambda})\)</span> yields the
posterior <span class="math inline">\(\psi^*(\lambda | \hat{\lambda})\)</span>:
<span class="math display" id="eq:post">\[\begin{equation}
\psi^*(\lambda | \hat{\lambda}) = \frac{1}{c} \sum_{r=0}^{\infty}\frac{(n^r/2)(\lambda/2)^{\nu_1/2+r-1}
e^{-(n+1)\lambda/2}}{B[(n-\nu_1)/2, \ \nu_1/2+r]\Gamma(\nu_1/2)r!}m^{\nu_1/2+r}
\left(\frac{1}{1+m\hat{\lambda}}\right)^{n/2+r}\hat{\lambda}^{\nu_1/2+r-1}, \  \lambda \geq 0.
\tag{2.21}
\end{equation}\]</span></p>
<p>Here <span class="math inline">\(B(.,\ .)\)</span> and <span class="math inline">\(\Gamma(.)\)</span> are the beta and gamma function respectively, <span class="math inline">\(c\)</span> is the norming constant
which is obtained through numeric integration of <a href="#eq:post">(2.21)</a> over <span class="math inline">\(0 &lt; \lambda &lt; \infty\)</span>.</p>
<p>Sums over infinite ranges are problematic for practical computation. One way do deal with this
is to set reasonable endpoints instead of infinity. For the implementation in <code>singcar</code> the endpoint of the
infinite sum was set by noting that the denominator in (<a href="#eq:post">(2.21)</a>)
goes to infinity faster than the numerator. In fact, in <code>R</code> and many other languages
<span class="math inline">\(r! = \infty\)</span> for <span class="math inline">\(r &gt; 170\)</span>, so a reasonable boundary for the infinite sum would then be <span class="math inline">\(170\)</span>,
because anything divided by infinity is 0. However, for some parameter values infinity will be
approached in the numerator before it is approached in the denominator. Infinity divided
by any real value is still infinity but infinity divided by infinity is undefined.
Therefore, to be able to sum, we remove any infinite or undefined value if present.
As such, the integral over the sum in (<a href="#eq:post">(2.21)</a>) will always be convergent and the normalising constant
<span class="math inline">\(c\)</span> can be calculated.</p>
<p><code>singcar</code> uses the function <code>integrate</code> in the base <code>R</code> package <code>stats</code> to do this. This
numerical integration procedure, based on routines from the QUADPACK package <span class="citation">(<a href="#ref-piessens2012quadpack" role="doc-biblioref">Piessens et al. 2012</a>)</span>,
often yields correct integrals. However, it uses subdivision of the interval to be integrated
and when integrating to infinity the area of interest will be undersampled. Hence,
in <code>singcar</code> we set:
<span class="math display">\[
c = \int_0^{0.5\sqrt{\hat\lambda}}\psi^*(\lambda | \hat{\lambda})  d\lambda + \int_{0.5\sqrt{\hat\lambda}}^{\sqrt{\hat\lambda}}\psi^*(\lambda | \hat{\lambda})  d\lambda +
\int_{\sqrt{\hat\lambda}}^{1.5\sqrt{\hat\lambda}}\psi^*(\lambda | \hat{\lambda})  d\lambda +
\int_{1.5\sqrt{\hat\lambda}}^{2\sqrt{\hat\lambda}}\psi^*(\lambda | \hat{\lambda})  d\lambda +
\int_{2\sqrt{\hat\lambda}}^{\infty}\psi^*(\lambda | \hat{\lambda}) \ d\lambda.
\]</span>
This ensures heavier sampling around <span class="math inline">\(\sqrt{\hat\lambda}\)</span>, which of course is the area we are most
interested in. Further, for large values of <span class="math inline">\(\hat\lambda\)</span> some discontinuities will be introduced
in the posterior which can be problematic for some numerical integration techniques as well but
the above subdivision will alleviate this problem somewhat.</p>
<p>The q:th quantile of the posterior <span class="math inline">\(\psi^*(\lambda | \hat{\lambda})\)</span>, <span class="math inline">\(\lambda_q\)</span> is then
found by an optimisation search procedure (<code>nlminb</code> in <code>stats</code>) that aims to minimise
<span class="math display">\[
\left| \int_{\lambda_0}^{\lambda_q} \psi^*(\lambda | \hat{\lambda})d\lambda - q \right|.
\]</span></p>
<p>To get an estimate of <span class="math inline">\(p\)</span> we use the same procedure to find the median of the posterior, <span class="math inline">\(\lambda_{0.5}\)</span>.
<span class="citation">Elfadaly, Garthwaite, and Crawford (<a href="#ref-elfadalyPointEstimationAbnormality2016" role="doc-biblioref">2016</a>)</span>’s modified estimator of <span class="math inline">\(p\)</span>, <span class="math inline">\(\hat{p}_{MD}\)</span>, is then:
<span class="math display">\[
\hat{p}_{MD} = min\left[\hat{p}_D,  \ 1-G\left(\lambda_{0.5}\right) \right].
\]</span>
Note however that due to the fact that <span class="math inline">\(c\)</span> will sometimes be very small and hence <span class="math inline">\(1/c\)</span>
very large, computations of the normalised posterior can be very slow, but
this mostly happens when <span class="math inline">\(\hat\delta\)</span> is so large that <span class="math inline">\(\hat{p}_D\)</span> would
be greater than <span class="math inline">\(1-G(\lambda_{0.5})\)</span> anyway. In <code>singcar</code> it is therefore recommended
to use <span class="math inline">\(\hat{p}_{D}\)</span> but to calculate <span class="math inline">\(\hat{p}_{MD}\)</span> if the Mahalanobis distance
of the case seems suspiciously small.</p>
<p>The difference between the estimators <span class="math inline">\(\hat{p}_D\)</span> and <span class="math inline">\(\hat{p}_{MD}\)</span> is small and
<span class="citation">Elfadaly, Garthwaite, and Crawford (<a href="#ref-elfadalyPointEstimationAbnormality2016" role="doc-biblioref">2016</a>)</span> show that their bias and means square error
are almost identical, but they recommend <span class="math inline">\(\hat{p}_{MD}\)</span>. This is because, as
<span class="math inline">\(\hat{\delta} \rightarrow 0, \ \hat{p}_D \rightarrow 1\)</span> much faster than it,
by common sense, should.</p>
<p>It should be noted that the implementation of <span class="math inline">\(\hat{p}_{MD}\)</span> in <code>singcar</code> is
unstable when compared to the implementation by
<span class="citation">Garthwaite, Elfadaly, and Crawford (<a href="#ref-garthwaiteModifiedConfidenceIntervals2017" role="doc-biblioref">2017</a>)</span> written in <code>C</code>. I have not been able
to find the cause of this discrepancy, but it only happens for relatively large
<span class="math inline">\(\hat{\delta}\)</span>. It is likely that the numerical integration procedures differ
and that the procedure used in the implementation by
<span class="citation">Garthwaite, Elfadaly, and Crawford (<a href="#ref-garthwaiteModifiedConfidenceIntervals2017" role="doc-biblioref">2017</a>)</span> better handles discontinuous
functions, which we see in the posterior (<a href="#eq:post">(2.21)</a>) for high values of
<span class="math inline">\(\hat{\delta}\)</span>. Alternatively, the discontinuities cause problems for the
optimisation routine. It is therefore recommended that both estimators are
calculated, and the smallest of the two are chosen, but if that is
<span class="math inline">\(\hat{p}_{MD}\)</span> then compare the result to the implementation by
<span class="citation">Garthwaite, Elfadaly, and Crawford (<a href="#ref-garthwaiteModifiedConfidenceIntervals2017" role="doc-biblioref">2017</a>)</span>.</p>
<!-- @garthwaiteModifiedConfidenceIntervals2017 argue that the proportion -->
<!-- of the control population that exhibit a larger $\delta$ than the case should not be -->
<!-- bigger than when $\boldsymbol{y}$ is a randomly drawn observation from the controls (## what -->
<!-- does this actually mean?).  -->
</div>
<div id="example-usage-in-singcar" class="section level3" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Example usage in <code>singcar</code></h3>
<p>Say that we want to know the case abnormality in the example data
on both tasks simultaneously. To estimate this abnormality on a
multivariate space, call:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">MTD</span>(<span class="at">case =</span> <span class="fu">c</span>(caseA, caseB), <span class="at">controls =</span> <span class="fu">cbind</span>(contA, contB),</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">conf_level =</span> <span class="fl">0.95</span>, <span class="at">method =</span> <span class="fu">c</span>(<span class="st">&quot;pd&quot;</span>, <span class="st">&quot;pchi&quot;</span>, <span class="st">&quot;pf&quot;</span>, <span class="st">&quot;pmd&quot;</span>),</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">mahalanobis_dist =</span> <span class="cn">NULL</span>, <span class="at">k =</span> <span class="cn">NULL</span>, <span class="at">n =</span> <span class="cn">NULL</span>)</span></code></pre></div>
<p>As can be seen, <span class="math inline">\(\hat{p}_D\)</span> is the default method to estimate the abnormality.
This is because the difference in results between <span class="math inline">\(\hat{p}_D\)</span> and <span class="math inline">\(\hat{p}_{MD}\)</span>
is very small, but the difference in efficiency is huge. In addition, <code>&quot;pmd&quot;</code> in
<code>singcar</code> is actually <span class="math inline">\(1-G(\lambda_{0.5})\)</span> rather than <span class="math inline">\(min[\hat{p}_D, 1-G(\lambda_{0.5})]\)</span>, hence taking the minimum is left to the user. The reason
for this was to minimise confusion. However, I realise that this could yield the
opposite effect and might be changed in future updates.</p>
<p>The <span class="math inline">\(p\)</span> value in the output of <code>MTD()</code> is the <span class="math inline">\(p\)</span> value associated
with the Hotelling’s <span class="math inline">\(T^2\)</span> statistic and the same as the abnormality
estimate if the method <code>pf</code> is chosen. The three last arguments are
only required if summary statistics are used.</p>
</div>
<div id="power" class="section level3" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Power calculators</h3>
<p>A further capacity of <code>singcar</code>is that it can be used to calculate
statistical power of the tests. The notion of power when comparing cases to
control samples have been somewhat overlooked for this class of statistical tests.
In recent work <span class="citation">(<a href="#ref-mcintoshPowerCalculationsSinglecase2020" role="doc-biblioref">McIntosh and Rittmo 2020</a>)</span>, we argued that,
even though power is inherently limited in this paradigm, a priori calculations are
still useful for study design and interpretation in neuropsychological and other applications.
Calculating power for the test of deficit is similar to calculating power for
any <span class="math inline">\(t\)</span> test and can be done analytically.
<span class="math display" id="eq:TDpower">\[\begin{equation} power = 1 - \beta =
T_{n-1}\left(t_{\alpha, \ n-1} \Bigg\rvert \frac{x^* - \overline{x}}{\sigma
\sqrt{\frac{n+1}{n}}}\right)
\tag{2.22}
\label{eq:TDpower}
\end{equation}\]</span>
Where <span class="math inline">\(T_{n-1}(.\rvert \theta)\)</span> is the cumulative distribution function for the
non-central <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom and non-centrality
parameter <span class="math inline">\(\frac{y^* - \overline{y}}{\sigma \sqrt{\frac{n+1}{n}}}\)</span> (i.e., TD, Equation
<a href="#eq:TD">(2.1)</a> and <span class="math inline">\(t_{\alpha, \ n-1}\)</span> is the <span class="math inline">\(\alpha\)</span> quantile of the
<span class="math inline">\(t\)</span> distribution on <span class="math inline">\(n-1\)</span> degrees of freedom (note that this is for a one-sided
test). For the unstandardised difference test power is calculated in an
analogous way by putting Equation <a href="#eq:UDT">(2.3)</a> as the non-centrality parameter. Deriving
power for the other functions in an analytic manner is however not possible (the
RSDT is only approximately <span class="math inline">\(t\)</span> distributed) and a Monte Carlo approach has been
used for these tests. To call any power calculator in the package one simply
uses the function names with <code>_power</code> added as a suffix.</p>
<p>So, for example, to calculate power for the test of deficit we call <code>TD_power()</code>.
The expected case score and either sample size or desired power must
be supplied. The mean and standard deviation of the control sample
can also be specified with the arguments <code>mean</code> and <code>sd</code>.
If not, they take the default values of 0 and 1 respectively so
that the case score is interpreted as distance from the mean
in standard deviations. A conventional <span class="math inline">\(\alpha\)</span>-level of
<span class="math inline">\(0.05\)</span> is assumed if nothing else is supplied. The alternative
hypothesis can also be specified by the argument <code>alternative</code>:
specify <code>&quot;less&quot;</code> (default) or <code>&quot;greater&quot;</code> for a one-tailed test, specify
<code>&quot;two.sided&quot;</code> for a two-tailed test.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">TD_power</span>(<span class="at">case =</span> <span class="dv">70</span>,</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">mean =</span> <span class="dv">100</span>,</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">sd =</span> <span class="dv">15</span>,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">sample_size =</span> <span class="dv">16</span>,</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">power =</span> <span class="cn">NULL</span>,</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">alternative =</span> <span class="st">&quot;less&quot;</span>,</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">alpha =</span> <span class="fl">0.05</span>,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">spec =</span> <span class="fl">0.005</span>)</span></code></pre></div>
<pre><code>## [1] 0.5819579</code></pre>
<p><code>TD_power()</code> can also be used to calculate required sample size for a
desired level of power. For example, if we specify a desired power level of 0.6,
leave <code>sample_size</code> to its default and let the rest of the arguments
be as in the previous example, we see from the output of the function that
power will not increase more than 0.5% for any additional participant after a sample
size of 15. That is, the algorithm stops searching
when this level of specificity has been reached and we are nearing the
asymptotic maximum power for this effect size. We can increase the specificity
by lowering the <code>spec</code> argument.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">TD_power</span>(<span class="at">case =</span> <span class="dv">70</span>,</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">mean =</span> <span class="dv">100</span>,</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">sd =</span> <span class="dv">15</span>,</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">sample_size =</span> <span class="cn">NULL</span>,</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">power =</span> <span class="fl">0.6</span>,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">alternative =</span> <span class="st">&quot;less&quot;</span>,</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">alpha =</span> <span class="fl">0.05</span>,</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">spec =</span> <span class="fl">0.005</span>)</span></code></pre></div>
<pre><code>## Power (0.578) will not increase more than 0.5% per participant for n &gt; 15</code></pre>
<pre><code>##    n     power
## 1 15 0.5780555</code></pre>
<p>Power calculators for the Bayesian tests of deficit cannot calculate
required sample size. This is because they rely on simulation methods
to estimate approximate power and deploying a search algorithm to find the required sample
size for a given level of power would be computationally too intense. The syntax
is otherwise relatively similar to that of <code>TD_power()</code>. For <code>BTD_power()</code>
we have the two extra arguments <code>nsim</code> and <code>iter</code>, indicating the number
of simulations used in the power function and by <code>BTD()</code>, respectively.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">BTD_power</span>(<span class="at">case =</span> <span class="dv">70</span>,</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">mean =</span> <span class="dv">100</span>,</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">sd =</span> <span class="dv">15</span>,</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">sample_size =</span> <span class="dv">15</span>,</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">alternative =</span> <span class="st">&quot;less&quot;</span>,</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">alpha =</span> <span class="fl">0.05</span>,</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">nsim =</span> <span class="dv">1000</span>,</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">iter =</span> <span class="dv">1000</span>)</span></code></pre></div>
<pre><code>## [1] 0.574</code></pre>
<p>The only difference in syntax of <code>BTD_cov_power()</code> is due to the inclusion of covariates.
The variate of interest must be specified as a vector where the first element
gives the mean and the second the standard deviation in the argument <code>control_task</code>.
The covariates can be specified similarly or as an <span class="math inline">\(m \times 2\)</span> matrix where the first
column gives the means of each covariate and the second column gives the standard
deviations. The correlation matrix of the variates must be given as well. In the example
below, power is evaluated for a test taking two covariates into account, both with a mean
of 0 and a standard deviation of 1. The correlation is specified as a <span class="math inline">\(3\times 3\)</span>
matrix with pairwise correlations of <span class="math inline">\(0.3\)</span>. The default settings include only one
covariate having a <span class="math inline">\(0.3\)</span> correlation with the variate of interest.
This function is computationally intense and hence, the number
of simulations has, for the example below, been decreased.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>covars <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>,</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>                   <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="fu">BTD_cov_power</span>(<span class="at">case =</span> <span class="sc">-</span><span class="dv">2</span>,</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">case_cov =</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="sc">-</span><span class="fl">0.6</span>),</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">control_task =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">control_covar =</span> covars,</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>              <span class="at">cor_mat =</span> <span class="fu">diag</span>(<span class="dv">3</span>) <span class="sc">+</span> <span class="fl">0.3</span> <span class="sc">-</span> <span class="fu">diag</span>(<span class="fu">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>)),</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>              <span class="at">sample_size =</span> <span class="dv">15</span>,</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>              <span class="at">alternative =</span> <span class="st">&quot;less&quot;</span>,</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>              <span class="at">alpha =</span> <span class="fl">0.05</span>,</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>              <span class="at">nsim =</span> <span class="dv">100</span>,</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>              <span class="at">iter =</span> <span class="dv">100</span>)</span></code></pre></div>
<pre><code>## [1] 0.49</code></pre>
<p>For the difference tests one must supply the expected case scores on both
variates as well as sample size. The means and standard deviations of the
control sample can also be specified. If unspecified, they take on the default values of
0 and 1 respectively, so that the expected case scores are interpreted as
distances from the means in standard deviations. <code>RSDT_power()</code>,
<code>BSDT_power()</code> and <code>UDT_power()</code> additionally require an estimate of
the sample correlation between the variates of interest, <code>r_ab</code>. If this is
not specified a correlation of 0.5 is assumed by default. For
<code>BSDT_cov_power()</code> the correlation matrix between the variates of interest
and the covariates must instead be supplied (i.e., at least a <span class="math inline">\(3\times3\)</span> matrix
where the first correlation is the correlation between the variates of
interest).</p>
<p>The alternative hypothesis is by default assumed to be
<code>&quot;two.sided&quot;</code> since the direction of the effect is dependent on the
order of the inputs, but can be specified to be <code>&quot;less&quot;</code> or
<code>&quot;greater&quot;</code> as well. The syntax is similar for all three functions but with small
differences. For <code>UDT_power()</code> one can request required sample size for a
desired power, as for <code>TD_power()</code>. Calculators for the Bayesian tests
have the extra argument <code>calibrated</code> as to be able to specify the prior.
<code>BSDT_cov_power()</code> requires input in the same format as <code>BTD_cov_power()</code>
for both <code>control_tasks</code> and <code>control_covar</code>. The two examples
below demonstrate usage for <code>RSDT_power()</code> and <code>BSDT_cov_power()</code>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">RSDT_power</span>(<span class="at">case_a =</span> <span class="dv">70</span>,</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">case_b =</span> <span class="dv">55</span>,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">mean_a =</span> <span class="dv">100</span>,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">mean_b =</span> <span class="dv">50</span>,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">sd_a =</span> <span class="dv">15</span>,</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">sd_b =</span> <span class="dv">10</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">r_ab =</span> <span class="fl">0.5</span>,</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">sample_size =</span> <span class="dv">15</span>,</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>,</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">alpha =</span> <span class="fl">0.05</span>,</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>           <span class="at">nsim =</span> <span class="dv">1000</span>)</span></code></pre></div>
<pre><code>## [1] 0.604</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>cor_mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,   <span class="fl">0.5</span>, <span class="fl">0.6</span>,</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.5</span>,   <span class="dv">1</span>, <span class="fl">0.3</span>,</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.6</span>, <span class="fl">0.3</span>,   <span class="dv">1</span>), <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="fu">BSDT_cov_power</span>(<span class="at">case_tasks =</span> <span class="fu">c</span>(<span class="dv">70</span>, <span class="dv">55</span>),</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">case_cov =</span> <span class="dv">65</span>,</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">control_tasks =</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">15</span>,</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>                                        <span class="dv">50</span>, <span class="dv">10</span>), <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>),</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>               <span class="at">control_covar =</span> <span class="fu">c</span>(<span class="dv">50</span>, <span class="dv">25</span>),</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>               <span class="at">cor_mat =</span> cor_mat,</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">sample_size =</span> <span class="dv">15</span>,</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>               <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>,</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>               <span class="at">alpha =</span> <span class="fl">0.05</span>,</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>               <span class="at">nsim =</span> <span class="dv">100</span>,</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>               <span class="at">iter =</span> <span class="dv">100</span>,</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>               <span class="at">calibrated =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## [1] 0.76</code></pre>
</div>
</div>
</div>
<div id="summary" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Summary</h1>
<p>This vignette has reviewed several statistical methods to compare a single case to
a small control sample. It has exemplified practical usage of these methods with
implementations in the package,
where possible, and in <code>lme4</code> where not. Using repeated measures data and linear
mixed models has been shown to potentially produce higher power when testing for
a discrepancy between two variates than the more standard RSDT, in a very small
simulation study. However, when no discrepancy is present but the case exhibits
severe impairments on both variates the mixed model yielded an extremely high
error rate, for the specific scenario investigated. So when estimating dissociations
it is recommended to use some of the Bayesian tests and aggregating the data.</p>
<p>Since more complex model structures can be created with linear mixed models
compared to the more standard procedures it is unfortunate that their usage for
finding dissociations seems limited. This is on the other hand somewhat made up
for by the implementation of the Bayesian regression techniques in Section
<a href="#cov">2.3.4</a>. In addition, implementation of (relatively) unbiased techniques
to estimate case abnormality in multidimensional space, Section <a href="#section4">2.5</a>,
also offers the possibility of increased complexity.</p>
<p>The methods described have been developed for keeping transparent control over
Type I errors, but power calculators have been implemented in the
package as well. Consideration of power can assist researchers in study design
and in setting realistic expectations for what these types of statistical
hypothesis tests can achieve <span class="citation">(<a href="#ref-mcintoshPowerCalculationsSinglecase2020" role="doc-biblioref">McIntosh and Rittmo 2020</a>)</span>.</p>
</div>
<div id="references" class="section level1" number="4">
<h1><span class="header-section-number">4</span> References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-baayenMixedeffectsModelingCrossed2008" class="csl-entry">
Baayen, R. H., D. J. Davidson, and D. M. Bates. 2008. <span>“Mixed-Effects Modeling with Crossed Random Effects for Subjects and Items.”</span> <em>Journal of Memory and Language</em> 59 (4): 390–412. <a href="https://doi.org/10.1016/j.jml.2007.12.005">https://doi.org/10.1016/j.jml.2007.12.005</a>.
</div>
<div id="ref-batesFittingLinearMixedeffects2015" class="csl-entry">
Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. <span>“Fitting Linear Mixed-Effects Models Using Lme4.”</span> <em>Journal of Statistical Software</em> 67 (1): 1–48. <a href="https://doi.org/10.18637/jss.v067.i01">https://doi.org/10.18637/jss.v067.i01</a>.
</div>
<div id="ref-bergerObjectivePriorsBivariate2008" class="csl-entry">
Berger, James O., and Dongchu Sun. 2008. <span>“Objective <span>Priors</span> for the <span>Bivariate</span> <span>Normal</span> <span>Model</span>.”</span> <em>The Annals of Statistics</em> 36 (2): 963–82.
</div>
<div id="ref-buckinghamGettingGripHeaviness2014" class="csl-entry">
Buckingham, Gavin. 2014. <span>“Getting a Grip on Heaviness Perception: A Review of Weight Illusions and Their Probable Causes.”</span> <em>Experimental Brain Research</em> 232 (6): 1623–29. <a href="https://doi.org/10.1007/s00221-014-3926-9">https://doi.org/10.1007/s00221-014-3926-9</a>.
</div>
<div id="ref-cohenStatisticalPowerAnalysis1988" class="csl-entry">
Cohen, J. 1988. <em>Statistical <span>Power</span> <span>Analysis</span> for the <span>Behavioral</span> <span>Sciences</span></em>. Lawrence Erlbaum Associates.
</div>
<div id="ref-crawfordInvestigationSingleCase2002" class="csl-entry">
Crawford, John, and Paul Garthwaite. 2002. <span>“Investigation of the Single Case in Neuropsychology: Confidence Limits on the Abnormality of Test Scores and Test Score Differences.”</span> <em>Neuropsychologia</em> 40 (8): 1196–1208. <a href="https://doi.org/10.1016/S0028-3932(01)00224-X">https://doi.org/10.1016/S0028-3932(01)00224-X</a>.
</div>
<div id="ref-crawfordTestingSuspectedImpairments2005" class="csl-entry">
———. 2005. <span>“Testing for <span>Suspected</span> <span>Impairments</span> and <span>Dissociations</span> in <span>Single</span>-<span>Case</span> <span>Studies</span> in <span>Neuropsychology</span>: <span>Evaluation</span> of <span>Alternatives</span> <span>Using</span> <span>Monte</span> <span>Carlo</span> <span>Simulations</span> and <span>Revised</span> <span>Tests</span> for <span>Dissociations</span>.”</span> <em>Neuropsychology</em> 19 (3): 318–31. <a href="https://doi.org/10.1037/0894-4105.19.3.318">https://doi.org/10.1037/0894-4105.19.3.318</a>.
</div>
<div id="ref-crawfordMethodsTestingDeficit2006" class="csl-entry">
———. 2006. <span>“Methods of Testing for a Deficit in Single-Case Studies: <span>Evaluation</span> of Statistical Power by <span>Monte</span> <span>Carlo</span> Simulation.”</span> <em>Cognitive Neuropsychology</em> 23 (6): 877–904. <a href="https://doi.org/10.1080/02643290500538372">https://doi.org/10.1080/02643290500538372</a>.
</div>
<div id="ref-crawfordComparisonSingleCase2007" class="csl-entry">
———. 2007. <span>“Comparison of a Single Case to a Control or Normative Sample in Neuropsychology: <span>Development</span> of a <span>Bayesian</span> Approach.”</span> <em>Cognitive Neuropsychology</em> 24 (4): 343–72. <a href="https://doi.org/10.1080/02643290701290146">https://doi.org/10.1080/02643290701290146</a>.
</div>
<div id="ref-crawfordSinglecaseResearchNeuropsychology2012" class="csl-entry">
———. 2012. <span>“Single-Case Research in Neuropsychology: <span>A</span> Comparison of Five Forms of t-Test for Comparing a Case to Controls.”</span> <em>Cortex</em> 48 (8): 1009–16. <a href="https://doi.org/10.1016/j.cortex.2011.06.021">https://doi.org/10.1016/j.cortex.2011.06.021</a>.
</div>
<div id="ref-crawfordComparingSingleCase2009" class="csl-entry">
Crawford, John, Paul Garthwaite, and D Howell. 2009. <span>“On Comparing a Single Case with a Control Sample: <span>An</span> Alternative Perspective.”</span> <em>Neuropsychologia</em> 47 (13): 2690–95. <a href="https://doi.org/10.1016/j.neuropsychologia.2009.04.011">https://doi.org/10.1016/j.neuropsychologia.2009.04.011</a>.
</div>
<div id="ref-crawfordInferentialMethodsComparing2004" class="csl-entry">
Crawford, John, Paul Garthwaite, D Howell, and C Gray. 2004. <span>“Inferential Methods for Comparing a Single Case with a Control Sample: Modified t‐tests Versus Mycroft Et Al.’s (2002) Modified Anova.”</span> <em>Cognitive Neuropsychology</em> 21 (7): 750–55. <a href="https://doi.org/10.1080/02643290342000276">https://doi.org/10.1080/02643290342000276</a>.
</div>
<div id="ref-crawfordPointIntervalEstimates2010" class="csl-entry">
Crawford, John, Paul Garthwaite, and S Porter. 2010. <span>“Point and Interval Estimates of Effect Sizes for the Case-Controls Design in Neuropsychology: <span>Rationale</span>, Methods, Implementations, and Proposed Reporting Standards.”</span> <em>Cognitive Neuropsychology</em> 27 (3): 245–60. <a href="https://doi.org/10.1080/02643294.2010.513967">https://doi.org/10.1080/02643294.2010.513967</a>.
</div>
<div id="ref-crawfordComparingSingleCase2011" class="csl-entry">
Crawford, John, Paul Garthwaite, and K Ryan. 2011. <span>“Comparing a Single Case to a Control Sample: <span>Testing</span> for Neuropsychological Deficits and Dissociations in the Presence of Covariates.”</span> <em>Cortex</em> 47 (10): 1166–78. <a href="https://doi.org/10.1016/j.cortex.2011.02.017">https://doi.org/10.1016/j.cortex.2011.02.017</a>.
</div>
<div id="ref-crawfordComparingIndividualTest1998" class="csl-entry">
Crawford, John, and D Howell. 1998. <span>“Comparing an <span>Individual</span>’s <span>Test</span> <span>Score</span> <span>Against</span> <span>Norms</span> <span>Derived</span> from <span>Small</span> <span>Samples</span>.”</span> <em>The Clinical Neuropsychologist</em> 12 (4): 482–86. <a href="https://doi.org/10.1076/clin.12.4.482.7241">https://doi.org/10.1076/clin.12.4.482.7241</a>.
</div>
<div id="ref-crawfordPayneJonesRevisited1998" class="csl-entry">
Crawford, John, D Howell, and Paul Garthwaite. 1998. <span>“Payne and <span>Jones</span> <span>Revisited</span>: <span>Estimating</span> the <span>Abnormality</span> of <span>Test</span> <span>Score</span> <span>Differences</span> <span>Using</span> a <span>Modified</span> <span>Paired</span> <span>Samples</span> t <span>Test</span>.”</span> <em>Journal of Clinical and Experimental Neuropsychology</em> 20 (6): 898–905. <a href="https://doi.org/10.1076/jcen.20.6.898.1112">https://doi.org/10.1076/jcen.20.6.898.1112</a>.
</div>
<div id="ref-dijkermanVisuomotorPerformancePatient2004a" class="csl-entry">
Dijkerman, H.Chris, Sandra Lê, Jean-François Démonet, and A.David Milner. 2004. <span>“Visuomotor Performance in a Patient with Visual Agnosia Due to an Early Lesion.”</span> <em>Cognitive Brain Research</em> 20 (1): 12–25. <a href="https://doi.org/10.1016/j.cogbrainres.2003.12.007">https://doi.org/10.1016/j.cogbrainres.2003.12.007</a>.
</div>
<div id="ref-elfadalyPointEstimationAbnormality2016" class="csl-entry">
Elfadaly, Fadlalla G., Paul H. Garthwaite, and John R. Crawford. 2016. <span>“On Point Estimation of the Abnormality of a <span>Mahalanobis</span> Index.”</span> <em>Computational Statistics &amp; Data Analysis</em> 99 (July): 115–30. <a href="https://doi.org/10.1016/j.csda.2016.01.014">https://doi.org/10.1016/j.csda.2016.01.014</a>.
</div>
<div id="ref-franePowerTypeError2015" class="csl-entry">
Frane, Andrew V. 2015. <span>“Power and <span>Type</span> <span>I</span> <span>Error</span> <span>Control</span> for <span>Univariate</span> <span>Comparisons</span> in <span>Multivariate</span> <span>Two</span>-<span>Group</span> <span>Designs</span>.”</span> <em>Multivariate Behavioral Research</em> 50 (2): 233–47. <a href="https://doi.org/10.1080/00273171.2014.968836">https://doi.org/10.1080/00273171.2014.968836</a>.
</div>
<div id="ref-garthwaiteDistributionDifferenceTwo2004" class="csl-entry">
Garthwaite, Paul, and John Crawford. 2004. <span>“The Distribution of the Difference Between Two t -Variates.”</span> <em>Biometrika</em> 91 (4): 987–94.
</div>
<div id="ref-garthwaiteModifiedConfidenceIntervals2017" class="csl-entry">
Garthwaite, Paul, Fadlalla G. Elfadaly, and John R. Crawford. 2017. <span>“Modified Confidence Intervals for the <span>Mahalanobis</span> Distance.”</span> <em>Statistics &amp; Probability Letters</em> 127 (August): 131–37. <a href="https://doi.org/10.1016/j.spl.2017.03.029">https://doi.org/10.1016/j.spl.2017.03.029</a>.
</div>
<div id="ref-gelmanBayesianDataAnalysis2013" class="csl-entry">
Gelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. 2013. <em>Bayesian <span>Data</span> <span>Analysis</span>, <span>Third</span> <span>Edition</span></em>. Chapman &amp; <span>Hall</span>/<span>CRC</span> <span>Texts</span> in <span>Statistical</span> <span>Science</span>. Taylor &amp; Francis. <a href="https://books.google.se/books?id=ZXL6AQAAQBAJ">https://books.google.se/books?id=ZXL6AQAAQBAJ</a>.
</div>
<div id="ref-hassanSizeweightIllusionVisual2020" class="csl-entry">
Hassan, Eleanor K, Anna Sedda, Gavin Buckingham, and Robert D McIntosh. 2020. <span>“The Size-Weight Illusion in Visual Form Agnosic Patient <span>DF</span>.”</span> <em>Neurocase</em>, August, 1–8. <a href="https://doi.org/10.1080/13554794.2020.1800748">https://doi.org/10.1080/13554794.2020.1800748</a>.
</div>
<div id="ref-huberComparingSingleCase2015a" class="csl-entry">
Huber, Stefan, Elise Klein, Korbinian Moeller, and Klaus Willmes. 2015. <span>“Comparing a Single Case to a Control Group – <span>Applying</span> Linear Mixed Effects Models to Repeated Measures Data.”</span> <em>Cortex</em> 71 (October): 148–59. <a href="https://doi.org/10.1016/j.cortex.2015.06.020">https://doi.org/10.1016/j.cortex.2015.06.020</a>.
</div>
<div id="ref-huizengaMultivariateNormativeComparisons2007" class="csl-entry">
Huizenga, Hilde M., Harriet Smeding, Raoul P. P. P. Grasman, and Ben Schmand. 2007. <span>“Multivariate Normative Comparisons.”</span> <em>Neuropsychologia</em> 45 (11): 2534–42. <a href="https://doi.org/10.1016/j.neuropsychologia.2007.03.011">https://doi.org/10.1016/j.neuropsychologia.2007.03.011</a>.
</div>
<div id="ref-kuznetsovaLmerTestPackageTests2017" class="csl-entry">
Kuznetsova, Alexandra, Per B. Brockhoff, and Rune H. B. Christensen. 2017. <span>“<span class="nocase">lmerTest</span> Package: <span>Tests</span> in Linear Mixed Effects Models.”</span> <em>Journal of Statistical Software</em> 82 (13): 1–26. <a href="https://doi.org/10.18637/jss.v082.i13">https://doi.org/10.18637/jss.v082.i13</a>.
</div>
<div id="ref-mcintoshPowerCalculationsSinglecase2020" class="csl-entry">
McIntosh, Robert D., and Jonathan Ö. Rittmo. 2020. <span>“Power Calculations in Single-Case Neuropsychology: <span>A</span> Practical Primer.”</span> <em>Cortex</em> 135: 146–58. <a href="https://doi.org/10.1016/j.cortex.2020.11.005">https://doi.org/10.1016/j.cortex.2020.11.005</a>.
</div>
<div id="ref-payneStatisticsInvestigationIndividual1957" class="csl-entry">
Payne, R. W., and H. G. Jones. 1957. <span>“Statistics for the Investigation of Individual Cases.”</span> <em>Journal of Clinical Psychology</em> 13 (2): 115–21.
</div>
<div id="ref-piessens2012quadpack" class="csl-entry">
Piessens, Robert, Elise de Doncker-Kapenga, Christoph W Überhuber, and David K Kahaner. 2012. <em><span>QUADPACK</span>: <span>A</span> Subroutine Package for Automatic Integration</em>. Vol. 1. Springer Science &amp; Business Media.
</div>
<div id="ref-reiserConfidenceIntervalsMahalanobis2001" class="csl-entry">
Reiser, Benjamin. 2001. <span>“Confidence <span>Intervals</span> for the <span>Mahalanobis</span> <span>Distance</span>.”</span> <em>Communications in Statistics - Simulation and Computation</em> 30 (1): 37–45. <a href="https://doi.org/10.1081/SAC-100001856">https://doi.org/10.1081/SAC-100001856</a>.
</div>
<div id="ref-rittmoSingcarComparingSingle2021" class="csl-entry">
Rittmo, Jonathan, and Robert McIntosh. 2021. <span>“Singcar: <span>Comparing</span> Single Cases to Small Samples.”</span> Manual. <a href="https://CRAN.R-project.org/package=singcar">https://CRAN.R-project.org/package=singcar</a>.
</div>
<div id="ref-shalliceNeuropsychologyMentalStructure1988" class="csl-entry">
Shallice, Tim. 1988. <em>From Neuropsychology to Mental Structure</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-sokalBiometryPrinciplesPractice1981" class="csl-entry">
Sokal, R. R., and F. J. Rohlf. 1981. <em>Biometry: <span>The</span> <span>Principles</span> and <span>Practice</span> of <span>Statistics</span> in <span>Biological</span> <span>Research</span></em>. W. H. Freeman.
</div>
<div id="ref-tiaoBayesianEstimationMultivariate1964" class="csl-entry">
Tiao, George C., and Arnold Zellner. 1964. <span>“On the <span>Bayesian</span> <span>Estimation</span> of <span>Multivariate</span> <span>Regression</span>.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 26 (2): 277–85. <a href="https://doi.org/10.1111/j.2517-6161.1964.tb00560.x">https://doi.org/10.1111/j.2517-6161.1964.tb00560.x</a>.
</div>
<div id="ref-west2014linear" class="csl-entry">
West, Brady T, Kathleen B Welch, and Andrzej T Galecki. 2014. <em>Linear Mixed Models: A Practical Guide Using Statistical Software</em>. <span>Crc Press</span>.
</div>
<div id="ref-wunderlichExactConfidenceIntervals2015" class="csl-entry">
Wunderlich, A., F. Noo, B. D. Gallas, and M. E. Heilbrun. 2015. <span>“Exact <span>Confidence</span> <span>Intervals</span> for <span>Channelized</span> <span>Hotelling</span> <span>Observer</span> <span>Performance</span> in <span>Image</span> <span>Quality</span> <span>Studies</span>.”</span> <em>IEEE Transactions on Medical Imaging</em> 34 (2): 453–64. <a href="https://doi.org/10.1109/TMI.2014.2360496">https://doi.org/10.1109/TMI.2014.2360496</a>.
</div>
</div>
</div>
<div id="appendix" class="section level1 unnumbered">
<h1>Appendix</h1>
<p>Code for the function <code>powerLMM()</code> which evaluates power for the linear
mixed model used to test for the presence of a dissociation and
compares it to the power for either the RSDT or the BSDT, with data
distributed as described in Section <a href="#lmmexample">2.4.1</a>.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>powerLMM <span class="ot">&lt;-</span> <span class="cf">function</span>(nsim, <span class="at">case_impairment =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>                     <span class="at">compare_against =</span> <span class="fu">c</span>(<span class="st">&quot;RSDT&quot;</span>, <span class="st">&quot;BSDT&quot;</span>), <span class="at">alpha =</span> <span class="fl">0.05</span>) {</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">require</span>(lme4)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">require</span>(lmerTest)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">require</span>(MASS)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>  comptest <span class="ot">&lt;-</span> <span class="fu">match.arg</span>(compare_against)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>  pdt <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">length =</span> nsim)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>  plm <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">length =</span> nsim)</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>  simdata <span class="ot">&lt;-</span>  <span class="fu">expand.grid</span>(<span class="at">case =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">29</span>)),</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>                          <span class="at">item =</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>), <span class="at">condition =</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>))</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>  simdata<span class="sc">$</span>ID <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">30</span>, <span class="dv">4</span>)) </span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contrasts</span>(simdata<span class="sc">$</span>condition) <span class="ot">&lt;-</span> <span class="fu">contr.sum</span>(<span class="dv">2</span>)</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">80</span>, <span class="dv">50</span>, <span class="dv">20</span>)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">15</span><span class="sc">^</span><span class="dv">2</span>, <span class="dv">120</span>,    <span class="dv">45</span>,  <span class="dv">7</span>,</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">120</span>, <span class="dv">10</span><span class="sc">^</span><span class="dv">2</span>,    <span class="dv">11</span>, <span class="dv">15</span>,</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">45</span>,    <span class="dv">11</span>,  <span class="dv">10</span><span class="sc">^</span><span class="dv">2</span>, <span class="dv">40</span>,</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">7</span>,     <span class="dv">15</span>,    <span class="dv">40</span>, <span class="dv">5</span><span class="sc">^</span><span class="dv">2</span>), <span class="at">nrow =</span> <span class="dv">4</span>, <span class="at">byrow =</span> T)</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim){</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>    dv <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="dv">30</span>, <span class="at">mu =</span> mu, <span class="at">Sigma =</span> sigma)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>    dv[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> dv[<span class="dv">1</span>, ] <span class="sc">+</span> case_impairment</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>    simdata<span class="sc">$</span>dv <span class="ot">&lt;-</span> <span class="fu">c</span>(dv)</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>    model2 <span class="ot">&lt;-</span> <span class="fu">lmer</span>(dv <span class="sc">~</span> case<span class="sc">*</span>condition <span class="sc">+</span> (condition<span class="sc">|</span>ID) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span> item), <span class="at">data =</span> simdata)</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>    agg_data1 <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(dv[ ,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>    agg_data2 <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(dv[ ,<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (comptest <span class="sc">==</span> <span class="st">&quot;RSDT&quot;</span>){</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>      pdt[i] <span class="ot">&lt;-</span> <span class="fu">RSDT</span>(agg_data1[<span class="dv">1</span>], agg_data2[<span class="dv">1</span>], </span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>                     agg_data1[<span class="sc">-</span><span class="dv">1</span>], agg_data2[<span class="sc">-</span><span class="dv">1</span>])[[<span class="st">&quot;p.value&quot;</span>]]</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>      pdt[i] <span class="ot">&lt;-</span> <span class="fu">BSDT</span>(agg_data1[<span class="dv">1</span>], agg_data2[<span class="dv">1</span>], </span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>                     agg_data1[<span class="sc">-</span><span class="dv">1</span>], agg_data2[<span class="sc">-</span><span class="dv">1</span>])[[<span class="st">&quot;p.value&quot;</span>]]</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>    plm[i] <span class="ot">&lt;-</span> <span class="fu">summary</span>(model2)[[<span class="st">&quot;coefficients&quot;</span>]][<span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (comptest <span class="sc">==</span> <span class="st">&quot;RSDT&quot;</span>){</span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.frame</span>(<span class="at">RSDT =</span> <span class="fu">sum</span>(pdt <span class="sc">&lt;</span> <span class="fl">0.05</span>)<span class="sc">/</span>nsim, <span class="at">LMM =</span> <span class="fu">sum</span>(plm <span class="sc">&lt;</span> <span class="fl">0.05</span>)<span class="sc">/</span>nsim)</span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.frame</span>(<span class="at">BSDT =</span> <span class="fu">sum</span>(pdt <span class="sc">&lt;</span> <span class="fl">0.05</span>)<span class="sc">/</span>nsim, <span class="at">LMM =</span> <span class="fu">sum</span>(plm <span class="sc">&lt;</span> <span class="fl">0.05</span>)<span class="sc">/</span>nsim)</span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
